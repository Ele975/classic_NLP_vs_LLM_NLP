{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "PNuhw1xhjfd_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uctKYg_bjYAc",
        "outputId": "6c514279-6e0d-430a-8543-3b37d6d09a93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m922.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import copy\n",
        "import openai\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import Tree\n",
        "from nltk.stem import *\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "from spacy.attrs import ORTH, NORM\n",
        "from spacy import displacy\n",
        "from spacy.tokens import Doc\n",
        "from spacy.language import Language\n",
        "from spacy.symbols import ORTH\n",
        "from spacy.tokenizer import Tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hhqwx3e_jjCu",
        "outputId": "928fa972-d329-4f65-c01d-988fd3ed6f29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "2Pnjp0i--lNX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb61fd68-43f1-4f0b-9f55-25180f363e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing"
      ],
      "metadata": {
        "id": "awgfff6ejspz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT prompt (from input text)"
      ],
      "metadata": {
        "id": "rbAFRKG9-siw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "input text"
      ],
      "metadata": {
        "id": "fSTkDrvK_qaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "A company operates four departments. Each department employs employees. Each of the employees may or may not have one or more dependents. Each employee may or may not have an employment history.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "P8YtKqDP_rRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correct results of entities and attributes from true data (used for testing purposes to compute precision, recall and F1 score)"
      ],
      "metadata": {
        "id": "85pERzJxfxEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "true_res = {'member': ['memdid', 'name', 'zip', 'date'], 'membership type': ['mname', 'price', 'mid'], 'sale transaction': ['date', 'tid'], 'merchandise item': ['mrchid', 'price', 'name'], 'day pass': ['ID', 'date'], 'pass category': ['passcatid', 'pcname', 'price']}\n",
        "true_card = [['member', 'membership type'], ['memberships type', 'member'], ['member', 'day pass'], ['day pass', 'member'], ['day pass', 'pass category'], ['pass category', 'day pass'], ['member', 'sale transaction'], ['sale transaction', 'member'], ['sale transaction', 'merchandise item'], ['merchandise item', 'sale transaction']]\n",
        "true_unique = [[1, 'member','memdid'], [1,'membership type','mid'], [1,'day pass','ID'], [1,'pass category','passcatid'], [1,'sale transaction','tid'],[1,'merchandise item','mrchid']]"
      ],
      "metadata": {
        "id": "ZKdDF6WefaoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "openai key"
      ],
      "metadata": {
        "id": "jtVaSpHa-4FT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Key that gives an access to openai account\n",
        "openai.api_key = 'key_code'"
      ],
      "metadata": {
        "id": "CnBm9P-Z-3k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT 4.0 examples for few-shot prompting for text simplification"
      ],
      "metadata": {
        "id": "e35bxqMlj1W-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ex12\n",
        "ex1_input = \"\"\"\n",
        "The company is organised into departments. Each department has a unique name, a unique number, and a particular employee who manages and/or controls the department.\n",
        "We keep track of the start date when that employee began managing the department. A department may have several locations.\n",
        "A department controls a number of projects, each of which has a unique name, a unique number, and a single location.\n",
        "We store each employee’s name, Social Security number, address, salary, sex (gender), birth date and multiple phone numbers.\n",
        "An employee is assigned to one department, but may work on several projects, which are not necessarily controlled by the same department.\n",
        "We keep track of the current number of hours per week that an employee works on each project. We also keep track of the direct supervisor of each employee (who is another employee).\n",
        "We want to keep track of the dependents of each employee for insurance purposes. We keep each dependent’s first name, sex, number, birth date, and relationship to the employee.\n",
        "\"\"\"\n",
        "\n",
        "ex1_output = \"\"\"\n",
        "Each department has a name and number. The department’s name is unique. The department’s number is unique.\n",
        "The department is managed by a unique employee. The managing has a starting date. A department may have several locations.\n",
        "A department controls zero or more projects. The project has a name, number, and location. The project’s name is unique.\n",
        "The project’s number is unique. The employee has a Social Security Number, address, salary, sex, birth date and phone numbers.\n",
        "An employee is assigned to one department. An employee can work on several projects. The working has a current number of hours per week.\n",
        "An employee can supervise zero or many employees. An employee can have one or more dependents. A dependent has a first name, sex, number, birth date, and relationship.\n",
        "\"\"\"\n",
        "\n",
        "# ex11\n",
        "ex2_input = \"\"\"\n",
        "The database must store painters, paintings, and galleries information. Painters have a unique number, Name, and phone number.\n",
        "Paintings have a unique number, several colors, title and price. Galleries have a unique number, owner, phone number, commission rate and address.\n",
        "A painting is painted and created by a particular artist, and that painting is exhibited in a particular gallery.\n",
        "A gallery can exhibit at least one paintings, but each painting can be exhibited in only one gallery.\n",
        "Similarity, a painting is painted by a single painter, but each painter can paint many paintings.\n",
        "\"\"\"\n",
        "\n",
        "ex2_output = \"\"\"\n",
        "The database must store painters, paintings, and galleries.\n",
        "Painters have a number, name, and phone number. The painter’s number is unique. Paintings have a number, colors, title and price.\n",
        "The painting’s number is unique. Galleries have a number, owner, phone number, commission rate and address.\n",
        "The gallerie’s number is unique. A painting is painted by a unique artist, and the painting is exhibited in a unique gallery.\n",
        "A gallery can exhibit one or more paintings, but each painting can be exhibited in only one gallery.\n",
        "A painting is painted by a single painter, but each painter can paint many paintings.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QdQhNskmjyOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT prompt to simplify text in order to easier extract entities, attributes and relationships from complex english"
      ],
      "metadata": {
        "id": "biy1rRRx-7UT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parameter controlling randomness of the output\n",
        "temp = 0.0\n",
        "\n",
        "def get_completion(prompt, model=\"gpt-4\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temp, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ],
      "metadata": {
        "id": "wEki7Nuq-xio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt"
      ],
      "metadata": {
        "id": "f5Cw1-B9UK16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "requirements = f\"\"\"\n",
        "{text}\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "The task is to extract an Entity-relationship diagram from natural language requirements.\n",
        "To do that, these requirements should be simplified as much as possible for the extraction of entities,\n",
        "attributes and relationships (i.e. short and simple sentences, explicit subjects, etc.).\n",
        "Two examples of semplification are given below, where the 'input text' is the original text and the 'output text' is the simplified text:\n",
        "\n",
        "Input text:\n",
        "'\n",
        "{ex1_input}\n",
        "'\n",
        "\n",
        "Output text:\n",
        "'\n",
        "{ex1_output}\n",
        "'\n",
        "\n",
        "Input text:\n",
        "'\n",
        "{ex2_input}\n",
        "'\n",
        "\n",
        "Output text:\n",
        "'\n",
        "{ex2_output}\n",
        "'\n",
        "\n",
        "Simplify as shown above the following text delimited by triple backticks, remembering to not remove any information, creating short and simple sentences and pay attention to not repeat any sentence if there are synonyms.\n",
        "If there are words composed with '-' or '_' or in camel case, maintain only the main token (i.e. the possessed token, e.g. in 'person_id', maintain id).\n",
        "If in the sentence there is the possibility to choose among two verbs (e.g. 'a painter paints and/or creates many paintings'), maintaing only the verb used previously or after this sentence (in the case of 'a painter paints and/or creates many painting.\n",
        "A painting is painted by one painter', maintain only the verb 'paints', i.e. 'a painter paints many painting. A painting is painted by one painter').\n",
        "Return as output only the modified text without backticks or additional spaces or text:\n",
        "\n",
        "Text:\n",
        "```{requirements}```\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "aGehm_OY7C3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gpt_output():\n",
        "  answers = []\n",
        "  for i in range(1):\n",
        "      response = get_completion(prompt)\n",
        "      answers.append(response)\n",
        "\n",
        "  return answers[0]"
      ],
      "metadata": {
        "id": "jq5QFK7L6yZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean GPT output (remotion of additional spaces, backticks, etc.)"
      ],
      "metadata": {
        "id": "E7Wq0HQ4_IjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_gpt_output(text):\n",
        "  new_text = ''\n",
        "  i = 0\n",
        "  while i < len(text):\n",
        "    if i == '`' and i + 2 < len(text) and text[i+1] == '`' and text[i+2] == '`':\n",
        "      i += 3\n",
        "    elif (text[i] == '\\'' and i+1 < len(text) and text[i+1] == '\\n') or  (text[i] == '\\n' and i+1 < len(text) and text[i+1] == '\\n') or (text[i] == '\\n' and i+1 < len(text) and text[i+1] == '\\''):\n",
        "      i += 3\n",
        "    else:\n",
        "      new_text += text[i]\n",
        "      i += 1\n",
        "\n",
        "  return new_text"
      ],
      "metadata": {
        "id": "_KNCqf8F_Ryi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text pre-processing"
      ],
      "metadata": {
        "id": "McnCCEfv_w6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence segmentation\n",
        "\n",
        "First three methods not used since last one is the best one (for comparation purposes)"
      ],
      "metadata": {
        "id": "3O5bFbFH_5J6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- SENTENCE SEGMENTATION --------------------\n",
        "\n",
        "# write them lower and compare lower text with them -> etc?\n",
        "abbreviations = [ 'a.', 'b.', 'c.', 'd.', 'e.', 'f.', 'g.', 'h.' 'k', 'j.', 'i.', 'l.',\n",
        "                 'm.', 'n.', 'o.', 'p.', 'q.', 'r.', 's.', 't.', 'u.', 'v.', 'w.', 'x.', 'y.', 'z.', 'mr.', 'ms.', 'mrs.', 'dr.', 'prof.',\n",
        "                 'capt.', 'rev.', 'col.', 'sgt.', 'st.', 'u.s.', 'i.e.', 'e.g.', 'ca.', 'i.a.', 'no.', 'p.s.', 'aka.', 'fig.',\n",
        "                 'cit.', 'v.', 'vol.']\n",
        "\n",
        "# if period, space and uppercase or some variations then split\n",
        "def sentence_segmentation_one(text):\n",
        "    finalSegmentation = []\n",
        "    mergedParts = \"\"\n",
        "    for i in range(0, len(text)-2):\n",
        "        if ((text[i] == '.' and text[i+1] == ' ' and text[i+2].isupper())\n",
        "        or (i <= len(text)-4 and text[i] == '.' and text[i+1] == '\"'\n",
        "            and text[i+2] == ' ' and text[i+1].isupper())\n",
        "        or (i <= len(text - 4) and text[i] == '.' and text[i+1] == ' '\n",
        "            and text[i+2] == '\"' and text[i+3].isupper())\n",
        "        or (i > len(text)-3 and text[i] == '.')):\n",
        "            finalSegmentation.append(mergedParts)\n",
        "            mergedParts = \"\"\n",
        "        else:\n",
        "            mergedParts += text[i]\n",
        "    return finalSegmentation\n",
        "\n",
        "# if same as above but previous word is not an abbreviation, then split\n",
        "def sentence_segmentation_two(text):\n",
        "    finalSegmentation = []\n",
        "    mergedParts = \"\"\n",
        "    temp = \"\"\n",
        "    for i in range(0, len(text)-2):\n",
        "        if (((text[i] == '.' and text[i+1] == ' ' and text[i+2].isupper())\n",
        "        or (i <= len(text)-4 and text[i] == '.' and text[i+1] == '\"'\n",
        "            and text[i+2] == ' ' and text[i+1].isupper())\n",
        "        or (i <= len(text - 4) and text[i] == '.' and text[i+1] == ' '\n",
        "            and text[i+2] == '\"' and text[i+3].isupper())\n",
        "        or (i > len(text)-3 and text[i] == '.'))\n",
        "        and (temp.split()[-1].lower() not in abbreviations)):\n",
        "            finalSegmentation.append(mergedParts)\n",
        "            mergedParts = \"\"\n",
        "        else:\n",
        "            mergedParts += text[i]\n",
        "            temp += text[i]\n",
        "    return finalSegmentation\n",
        "\n",
        "def sentence_segmentation_three(text):\n",
        "    pattern = r'(?<=[.!?]) +(?=[A-Z])'\n",
        "    finalSegmentation = re.split(pattern, text)\n",
        "    return finalSegmentation\n",
        "\n",
        "\n",
        "\n",
        "# implementation of sent_tokenize + join sentences if abbreviations that doesn't define end of sentence\n",
        "def sentence_segmentation_mine(text):\n",
        "    # general sentence segmentation\n",
        "    firstSegmentation = sent_tokenize(text)\n",
        "    print(firstSegmentation)\n",
        "    finalSegmentation = []\n",
        "    i = 0\n",
        "    while i < len(firstSegmentation):\n",
        "    # for i in range(0, len(firstSegmentation)):\n",
        "      finalToken = firstSegmentation[i].split()[-1]\n",
        "      # if abbreviation at the end of a sentence split by nltk, merge current sentence with next sentence\n",
        "      if finalToken.lower() in abbreviations and i < len(firstSegmentation)-1:\n",
        "        segment = firstSegmentation[i] + ' ' + firstSegmentation[i+1]\n",
        "        # check for next sentences to merge (all subsequent sentences which ends with an abbreviation)\n",
        "        j = i+1\n",
        "        while j < len(firstSegmentation):\n",
        "          finalToken = firstSegmentation[j].split()[-1]\n",
        "          if finalToken.lower() in abbreviations:\n",
        "            segment += ' ' + firstSegmentation[j]\n",
        "            j += 1\n",
        "          # if no more sentences that end with abbreviation, store all merged sentences and restart from current last sentence\n",
        "          else:\n",
        "            finalSegmentation.append(segment)\n",
        "            i = j + 1\n",
        "            break\n",
        "      else:\n",
        "        finalSegmentation.append(firstSegmentation[i])\n",
        "        i += 1\n",
        "    return finalSegmentation"
      ],
      "metadata": {
        "id": "9fLRtX_5_0jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lower-case text transformation"
      ],
      "metadata": {
        "id": "3sZ3jNcU_92a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# id in lowercase is not recognise as a single token by spaCy, thus transform it in uppercase (recognised) if encountered\n",
        "\n",
        "def replace_word_with_uppercase(sentence, word):\n",
        "    # Define a regular expression pattern to match the word\n",
        "    pattern = re.compile(r'\\b{}\\b'.format(re.escape(word)))\n",
        "    # Replace the matched word with its uppercase version\n",
        "    replaced_sentence = pattern.sub(word.upper(), sentence)\n",
        "    return replaced_sentence\n",
        "\n",
        "\n",
        "def to_lowercase(sentences):\n",
        "  lowerText = []\n",
        "  for sentence in sentences:\n",
        "    lowerText.append(sentence.lower())\n",
        "  for i in range(0, len(lowerText)):\n",
        "    if 'id' in lowerText[i]:\n",
        "      lowerText[i] = replace_word_with_uppercase(lowerText[i], 'id')\n",
        "  return lowerText\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vsGxebik_7zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pronoun substitution\n",
        "\n",
        "Find nouns in the text, find pronouns and substitute them with antecedent subject"
      ],
      "metadata": {
        "id": "vjVNV1HDAFRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find all subjects in the text with the position of the first token\n",
        "\n",
        "subjs_noun = []\n",
        "\n",
        "def subject_extraction_for_pron(sentences):\n",
        "    for sentence in sentences:\n",
        "      temp_subj_nouns = []\n",
        "      doc = nlp(sentence)\n",
        "      i = 0\n",
        "      while i < len(doc):\n",
        "        # last condition works for composite words as 'booking dates'\n",
        "        if doc[i].pos_ == 'NOUN' or doc[i].pos_ == 'PROPN' or (doc[i].pos_ == 'VERB' and i+1 < len(doc) and doc[i].dep_ == 'compound'):\n",
        "          sentence = str(doc[i])\n",
        "          # save last token of noun info to know if it is a subject or not\n",
        "          subj_info = doc[i].dep_\n",
        "          # check if special case of 'first name', 'middle name', 'last name', 'serial number'\n",
        "          if i-1 >= 0 and (str(doc[i]) == 'name' and (str(doc[i-1]) == 'first' or str(doc[i-1]) == 'last' or str(doc[i-1]) == 'middle') or (str(doc[i-1]) == 'serial' and str(doc[i]) == 'number')):\n",
        "            sentence = str(doc[i-1]) + ' ' + str(doc[i])\n",
        "          # if noun as last token of the sentence insert in temp_nouns and insert temp_nouns to nouns at the end of the code and continue with next sentence\n",
        "          if i == len(doc) - 1 and (subj_info == 'nsubj' or subj_info == 'nsubjpass' or subj_info == 'csubj' or subj_info == 'csubjpass'):\n",
        "            temp_subj_nouns.append([sentence, i])\n",
        "            break\n",
        "          j = i + 1\n",
        "          while j < len(doc):\n",
        "            # handle manually problem of tags of 'ID' -> if compound with ID, first token doesn't have the compound tag, thus check only if next token is ID\n",
        "            if doc[j-1].dep_ == 'compound' and (doc[j].pos_ == 'NOUN' or doc[j].pos_ == 'PROPN' or doc[j].pos_ == 'VERB'):\n",
        "              sentence += ' ' + str(doc[j])\n",
        "              subj_info = doc[j].dep_\n",
        "              j+= 1\n",
        "            else:\n",
        "              if subj_info == 'nsubj' or subj_info == 'nsubjpass' or subj_info == 'csubj' or subj_info == 'csubjpass':\n",
        "                temp_subj_nouns.append([sentence, i])\n",
        "              i = j - 1\n",
        "              break\n",
        "        i+=1\n",
        "      subjs_noun.append(temp_subj_nouns)\n",
        "    return subjs_noun\n",
        "\n",
        "p_pronouns = ['i', 'me', 'she', 'her', 'he', 'him', 'you', 'they', 'them', 'we', 'us', 'it']\n",
        "\n",
        "def pronoun_substitution(sentences):\n",
        "  no_pronoun_text = []\n",
        "  # start from second sentence and check if there are some pronouns. If yes, get last subject of previous sentence and substitute pronoun with that subject\n",
        "  for i in range(0, len(sentences)):\n",
        "    temp_sentence = \"\"\n",
        "    doc = nlp(sentences[i])\n",
        "    j = 0\n",
        "    while j < len(doc):\n",
        "      if doc[j].pos_ == 'PRON' and str(doc[j]).lower() in p_pronouns:\n",
        "        # pick last subject from last sentence only if there is not a subject in the current sentence before the pronoun (pick previous closest subject)\n",
        "        # e.g. 'Anita loves eating and she is thin.'\n",
        "        if len(subjs_noun[i]) > 0:\n",
        "          prev_subj = ''\n",
        "          prev_subj_pos = -1\n",
        "          for elem in subjs_noun[i]:\n",
        "            if elem[1] < j and elem[1] > prev_subj_pos:\n",
        "              prev_subj = elem[0]\n",
        "              prev_subj_pos = elem[1]\n",
        "          # if previous subject found in current sentence, substitute, otherwise pick last subject in previous sentence\n",
        "          if prev_subj_pos != -1:\n",
        "            # if pronoun is at the beginning of the sentence, don't add a space before, otherwise yes\n",
        "            if j == 0:\n",
        "              temp_sentence = prev_subj\n",
        "            else:\n",
        "              temp_sentence += ' ' + prev_subj\n",
        "          else:\n",
        "            if i > 0:\n",
        "              prev_subj = subjs_noun[i-1][-1][0]\n",
        "              if j == 0:\n",
        "                temp_sentence = prev_subj\n",
        "              else:\n",
        "                temp_sentence += ' ' + prev_subj\n",
        "      else:\n",
        "        if j == 0:\n",
        "          temp_sentence = str(doc[j])\n",
        "        else:\n",
        "          temp_sentence += ' ' + str(doc[j])\n",
        "      j += 1\n",
        "\n",
        "    no_pronoun_text.append(temp_sentence)\n",
        "    temp_sentence = []\n",
        "  return no_pronoun_text\n"
      ],
      "metadata": {
        "id": "dgAJpLA2ACIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text lemmatization"
      ],
      "metadata": {
        "id": "vvPB0gEpboi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_lemmatization(sentences):\n",
        "  lem_text = []\n",
        "  for sentence in sentences:\n",
        "    doc = nlp(sentence)\n",
        "    lem_sentence = ' '.join([token.lemma_ for token in doc])\n",
        "    lem_text.append(lem_sentence)\n",
        "  return lem_text"
      ],
      "metadata": {
        "id": "rigYrokqbjxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handmade list of common attribute"
      ],
      "metadata": {
        "id": "Ia5MXIp5fXOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# taken from internet sources.\n",
        "# id in uppercase since we put it manually in this way\n",
        "common_attr = [\n",
        "    'name', 'last name', 'first name', 'middle', 'description', 'address', 'sex', 'ID', 'number',\n",
        "    'no', 'code', 'date', 'version', 'status', 'volume', 'birth', 'title', 'price',\n",
        "    'amount', 'value', 'phone number', 'isbn', 'birth date', 'enable', 'disable', 'label', 'title', 'size',\n",
        "    'length', 'width', 'height', 'weight', 'color', 'quantity', 'time', 'timestamp',\n",
        "    'start date', 'starting date', 'ending date', 'end date', 'start time', 'end time', 'due date', 'priority',\n",
        "    'latitude', 'longitude', 'phone', 'fax', 'url', 'comment', 'note', 'result', 'option', 'setting',\n",
        "    'permission', 'configuration', 'authentication', 'constraint', 'requirement', 'condition',\n",
        "    'date of birth', 'serial number', 'zip code', 'feedback', 'homepage url', 'webpage url', 'starting year',\n",
        "    'ending year', 'start year', 'end year'\n",
        "    ]"
      ],
      "metadata": {
        "id": "Yt5V-tG5fWl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extraction methods - Modeling part"
      ],
      "metadata": {
        "id": "zy7KQbFbbtW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Noun extraction"
      ],
      "metadata": {
        "id": "Pfzl2yZIb2Km"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- NOUNS EXTRACTION - BASIC --------------------\n",
        "\n",
        "# format -> [noun, starting position in sentence, nr of tokens which compose the noun, .pos_ of last token, .dep_ of last token, [array of .ent_type_ of each token in order], token.tag_]:\n",
        "  # 1. the string representing the noun (monogram, bigram, etc)\n",
        "  # 2. to easily retrieve the noun, store position of first token of the noun\n",
        "  # 3. to know how many tokens to jump to continue the sentence, store nr of tokens which compose the nouns\n",
        "  # 4. .pos_ = DET, NOUN, ADJ to know role of noun\n",
        "  # 5. .dep_ = nsubjpass, amod, det, etc., to know other information about the token (mainly to detect subjects of sentences)\n",
        "  # 6. .ent_type_ = PERSON, GPE, to remove proper names of people and locations\n",
        "  # 7. .tag_ = to identify if the noun is singular or plural (mainly for recognition of relationships based on possession)\n",
        "\n",
        "# triple array which separe nouns for each sentence => s1 = [[info1], [info2], ...].\n",
        "\n",
        "def nouns_extraction(sentences):\n",
        "  nouns = []\n",
        "  for sentence in sentences:\n",
        "    temp_nouns = []\n",
        "    doc = nlp(sentence)\n",
        "    # print(doc)\n",
        "    i = 0\n",
        "    while i < len(doc):\n",
        "      composed_of = False\n",
        "      # last condition works for composite words as booking dates\n",
        "      if doc[i].pos_ == 'NOUN' or doc[i].pos_ == 'PROPN' or ((doc[i].pos_ == 'VERB' or doc[i].pos_ == 'ADJ') and i+1 < len(doc) and doc[i].dep_ == 'compound'):\n",
        "        sentence = str(doc[i])\n",
        "        # print('doc[i]:', sentence, doc[i].pos_)\n",
        "\n",
        "        ent_type = [doc[i].ent_type_]\n",
        "        last_info = [sentence, i, 1, doc[i].pos_, doc[i].dep_, ent_type, doc[i].tag_]\n",
        "\n",
        "        # handle special case in which composite noun has 'of' -> level of difficulty, etc.\n",
        "        if i + 1 < len(doc) and str(doc[i+1]) == 'of' and (str(doc[i]) == 'level' or str(doc[i]) == 'degree' or str(doc[i]) == 'quality' or str(doc[i]) == 'state' or str(doc[i]) == 'form' or str(doc[i]) == 'number'):\n",
        "          last_info[0] = str(doc[i]) + ' ' + str(doc[i+1])\n",
        "          last_info[1] = i\n",
        "          last_info[2] = 2\n",
        "          last_info[5] = [doc[i+1].ent_type_] + last_info[5]\n",
        "          i += 1\n",
        "          composed_of = True\n",
        "        # check if special case of 'first name' or 'last name' or 'serial number'\n",
        "        if i-1 >= 0 and ((str(doc[i]) == 'name' and (str(doc[i-1]) == 'first' or str(doc[i-1]) == 'last' or str(doc[i-1]) == 'middle')) or (str(doc[i-1]) == 'serial' and str(doc[i]) == 'number') or (str(doc[i-1]) == 'starting' and str(doc[i]) == 'date') or (str(doc[i-1]) == 'ending' and str(doc[i]) == 'date')):\n",
        "          last_info[0] = str(doc[i-1]) + ' ' + str(doc[i])\n",
        "          last_info[1] = i-1\n",
        "          last_info[2] = 2\n",
        "          last_info[5] = [doc[i-1].ent_type_] + last_info[5]\n",
        "        # if noun as last token of the sentence insert in temp_nouns and insert temp_nouns to nouns at the end of the code and continue with next sentence\n",
        "        if i == len(doc) - 1:\n",
        "          temp_nouns.append(last_info)\n",
        "          break\n",
        "\n",
        "        j = i + 1\n",
        "\n",
        "        while j < len(doc):\n",
        "          # print('doc[j]:', doc[j])\n",
        "          if (doc[j-1].dep_ == 'compound' and (doc[j].pos_ == 'NOUN' or doc[j].pos_ == 'PROPN' or doc[j].pos_ == 'VERB')) or str(doc[j]) == 'ID' or (composed_of and (doc[j].pos_ == 'NOUN' or doc[j].pos_ == 'PROPN')) or ((doc[j-1].pos_ == 'NOUN' or doc[j-1].pos_ == 'PROPN') and (doc[j].pos_ == 'NOUN' or doc[j].pos_ == 'PROPN')):\n",
        "            last_info[0] += ' ' + str(doc[j])\n",
        "            last_info[2] += 1\n",
        "            last_info[3] = doc[j].pos_\n",
        "            last_info[4] = doc[j].dep_\n",
        "            last_info[5].append(doc[j].ent_type_)\n",
        "            last_info[6] = doc[j].tag_\n",
        "\n",
        "            j+= 1\n",
        "          elif str(doc[j]) == '-' and j+1 < len(doc) and (doc[j+1].pos_ == 'NOUN' or doc[j+1].pos_ == 'PROPN'):\n",
        "            last_info[0] += str(doc[j]) + str(doc[j+1])\n",
        "            last_info[2] += 2\n",
        "            last_info[3] = doc[j+1].pos_\n",
        "            last_info[4] = doc[j+1].dep_\n",
        "            last_info[5].append(doc[j].ent_type_)\n",
        "            last_info[5].append(doc[j+1].ent_type_)\n",
        "            last_info[6] = doc[j+1].tag_\n",
        "            j += 2\n",
        "          else:\n",
        "            i = j - 1\n",
        "            # if  not any(word in last_info[0] for word in db_nouns):\n",
        "            temp_nouns.append(last_info)\n",
        "            composed_of = False\n",
        "            break\n",
        "      composed_of = False\n",
        "      i+=1\n",
        "    nouns.append(temp_nouns)\n",
        "  return nouns"
      ],
      "metadata": {
        "id": "0qcd0XWAbxcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "People and locations proper names removal from noun array"
      ],
      "metadata": {
        "id": "xWf3LbvacHhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove from noun list -> use ent_type tag, if 'PERSON' or 'GPE' then remove. Remove only if entire noun is made of these two tags\n",
        "\n",
        "person_location_tags = ['PERSON', 'GPE']\n",
        "# specific nouns we need to maintain which tags are PERSON or GPE\n",
        "maintained_nouns = ['db']\n",
        "\n",
        "def proper_noun_removal(nouns_array):\n",
        "  new_nouns_array = []\n",
        "  for i in range(0, len(nouns_array)):\n",
        "    sentence_nouns = []\n",
        "    for j in range(0, len(nouns_array[i])):\n",
        "      nn = nouns_array[i][j].copy()\n",
        "      nr_tags = len(nouns_array[i][j][5])\n",
        "      remove = 0\n",
        "      if nr_tags > 0:\n",
        "        for tag in nouns_array[i][j][5]:\n",
        "          if tag == 'PERSON' or tag == 'GPE':\n",
        "            remove += 1\n",
        "        if remove < nr_tags or ((len(nouns_array[i][j]) > 0 and len(nouns_array[i][j][0].split('_')) > 1 and nouns_array[i][j][0].split('_')[-1].lower() == 'id') or nouns_array[i][j][0] in maintained_nouns):\n",
        "          sentence_nouns.append(nn)\n",
        "    new_nouns_array.append(sentence_nouns)\n",
        "    sentence_nouns = []\n",
        "  return new_nouns_array"
      ],
      "metadata": {
        "id": "dq_MzDgxcaxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noun lemmatization"
      ],
      "metadata": {
        "id": "Jv_29AF1fFry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def noun_lemmatization(nouns_array):\n",
        "  lemmatized_nouns = []\n",
        "  for i in range(0, len(nouns)):\n",
        "    temp = []\n",
        "    for j in range(0, len(nouns[i])):\n",
        "      noun = nouns[i][j].copy()\n",
        "      doc_nn = nlp(noun[0])\n",
        "      lem_noun = ' '.join([token.lemma_ for token in doc_nn])\n",
        "      noun[0] = lem_noun\n",
        "      temp.append(noun)\n",
        "    lemmatized_nouns.append(temp)\n",
        "    temp = []\n",
        "  return lemmatized_nouns"
      ],
      "metadata": {
        "id": "K0uR6__1fG4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entity type/attribute extraction"
      ],
      "metadata": {
        "id": "P6ilM1F1f6fx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract entity types and attributes based on the heuristic rule of the underscore (or dash).\n",
        "\n",
        "If entities and attributes in the form of ent_attr or ent-attr, e.g. student_id or student-id, remove first part which identifies the entity and associated with it the second part which identifies the attribute -> check first if it's not an attribute only (e.g. phone_number)"
      ],
      "metadata": {
        "id": "ZlhXI1SRgD2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_underscore(nouns, ent_attr):\n",
        "  for i in range(0, len(nouns)):\n",
        "    for j in range(0, len(nouns[i])):\n",
        "      noun = nouns[i][j]\n",
        "      if '_' in noun[0] or '-' in noun[0]:\n",
        "        split_noun = noun[0].split('_') if '_' in noun[0] else noun[0].split('-')\n",
        "        for z in range(0, len(split_noun)):\n",
        "          split_noun[z] = split_noun[z].strip()\n",
        "        # check if not completely an attribute\n",
        "        noun_str = ' '.join(split_noun)\n",
        "        if noun_str in common_attr:\n",
        "          continue\n",
        "        else:\n",
        "          # pick entity and attribute\n",
        "          entity = split_noun[0]\n",
        "          attribute = split_noun[1]\n",
        "          # put it in uppercase to be consistent\n",
        "          if attribute == 'id':\n",
        "            attribute = 'ID'\n",
        "          doc_entity = nlp(entity)\n",
        "          entity = ' '.join([token.lemma_ for token in doc_entity])\n",
        "          if entity not in ent_attr.keys():\n",
        "            ent_attr[entity] = []\n",
        "            ent_attr[entity].append(attribute)\n",
        "          else:\n",
        "            ent_attr[entity].append(attribute)\n",
        "  return ent_attr"
      ],
      "metadata": {
        "id": "6voSawlGgEn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract entity types and attributes from composite nouns\n",
        "\n",
        "If the second token or more of a composite noun is in the list of common attributes, insert first token as entitx type and following tokens and attributes"
      ],
      "metadata": {
        "id": "X1UfqI4OgLgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove one or more consecutive words (token/s which compose the attribute) from a string (the noun)\n",
        "def remove_consecutive_words(sentence, words):\n",
        "    pattern = r'\\b(?:' + '|'.join(re.escape(word) for word in words) + r')\\b(?:\\s+\\b(?:' + '|'.join(re.escape(word) for word in words) + r')\\b)*'\n",
        "    return re.sub(pattern, '', sentence)\n",
        "\n",
        "def extract_common(nouns, ent_attr):\n",
        "  for sentence in nouns:\n",
        "    for noun in sentence:\n",
        "      # there can be more correspondences in the common attributes, e.g. student phone number can correspond with 'number' and 'phone' and 'phone number', then the longest one win\n",
        "      equal_count = 0\n",
        "      winning_attr = ''\n",
        "      # check if noun is composite\n",
        "      if len(noun[0].split(' ')) > 1:\n",
        "        for attribute in common_attr:\n",
        "          if attribute in noun[0]:\n",
        "            if len(attribute.split(' ')) > equal_count:\n",
        "              equal_count = len(attribute.split(' '))\n",
        "              winning_attr = attribute\n",
        "        if equal_count != 0:\n",
        "          words_to_remove = winning_attr.split(' ')\n",
        "          noun_without_attr = remove_consecutive_words(noun[0], words_to_remove)\n",
        "          # avoid special case in which there is 'isbn value' where value is not important and should not be taken into consideration\n",
        "          # case in which the noun is empty too\n",
        "          if noun_without_attr != '' and noun_without_attr == 'isbn' and winning_attr == 'value':\n",
        "            if noun_without_attr in ent_attr.keys():\n",
        "              # don't add if attribute already in the list\n",
        "              if not (winning_attr in ent_attr[noun_without_attr]):\n",
        "                  ent_attr[noun_without_attr].append(winning_attr)\n",
        "            else:\n",
        "              ent_attr[noun_without_attr] = [winning_attr]\n",
        "  return ent_attr"
      ],
      "metadata": {
        "id": "rjgWavWugKut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract entities types based on the heuristic rule of subjects"
      ],
      "metadata": {
        "id": "ql6Gq_v-qrpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def subj_extraction(lemmatized_nouns):\n",
        "  subjects = []\n",
        "  for sentence in lemmatized_nouns:\n",
        "    temp_nouns = []\n",
        "    for noun in sentence:\n",
        "      if noun[4] == 'nsubj' or noun[4] == 'nsubjpass' or noun[4] == 'csubj' or noun[4] == 'csubjpass':\n",
        "        temp_nouns.append([noun[0], noun[-1], noun[1]])\n",
        "    subjects.append(temp_nouns)\n",
        "    temp_nouns = []\n",
        "  return subjects"
      ],
      "metadata": {
        "id": "YjhA16kyqtJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract entity type/entity type relationship or entitytype /attribute relationship based on quantity tokens"
      ],
      "metadata": {
        "id": "2CY6Vzh9DV_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# store cardinalities with the two entities or entity/attribute:\n",
        "# 1. [(1, 01),ent1, ent2] -> one-to-one (zero included)\n",
        "# 2. [(1, 1),ent1, ent2] -> one-to-one\n",
        "# 4. [(1, 02),ent1, ent2] -> one-to-many (zero included)\n",
        "# 3. [(1, 2),ent1, ent2] -> one-to-many\n",
        "\n",
        "# common quantity tokens\n",
        "quantity_adv = ['zero', 'one', 'unique', 'single', 'specific', 'sole', 'many', 'more', 'multiple', 'several', 'numerous', 'various', 'two']\n",
        "\n",
        "def quantity_extr(sentences, subjects, ent_attr, cardinality_attr, cardinality):\n",
        "  for i in range(0, len(sentences)):\n",
        "    doc = nlp(sentences[i])\n",
        "    # iterate over all subjects in the current sentence.\n",
        "    # starting from the subject, check if after it there is a verb, a quantity and other nouns. If yes, store the other nouns as entities\n",
        "    j = 0\n",
        "    while j < len(subjects[i]):\n",
        "      verb_found = False\n",
        "      quantity_found = False\n",
        "      noun_found = False\n",
        "      quantity = ''\n",
        "      snd_quantity = ''\n",
        "      other_subj = ''\n",
        "      verb = ''\n",
        "      attr_found = False\n",
        "      # if a new subject encountered, restart analysis (everything above)\n",
        "      restart = False\n",
        "      # look everything after subject -> position of first token of subject + its length + 1 to have next token\n",
        "      k = subjects[i][j][2] + len(subjects[i][j][0].split(' '))\n",
        "\n",
        "      while k < len(doc):\n",
        "        if restart:\n",
        "          break\n",
        "        # if encounter another subject, break loop and restart\n",
        "        elif (j + 1 < len(subjects[i]) and k == subjects[i][j+1][2]):\n",
        "          restart = True\n",
        "          verb_found = False\n",
        "          verb = ''\n",
        "          quantity_found = False\n",
        "          quantity = ''\n",
        "          snd_quantity = ''\n",
        "          other_subj = ''\n",
        "          break\n",
        "\n",
        "        # check if verb found\n",
        "        elif doc[k].pos_ == 'VERB' or (doc[k].pos_ == 'AUX' and str(doc[k]) == 'be') or (doc[k].dep_ == 'prt' and verb != ''):\n",
        "          verb_found = True\n",
        "          # store verb with relationship\n",
        "          if verb == '':\n",
        "            verb += str(doc[k])\n",
        "          else:\n",
        "            verb += ' ' + str(doc[k])\n",
        "\n",
        "        # check for quantity token and store it. Store only once (first time)\n",
        "        elif verb_found and str(doc[k]) in quantity_adv and not quantity_found:\n",
        "          quantity_found = True\n",
        "          quantity = str(doc[k])\n",
        "\n",
        "        # if previous quantity stored is 0, often associated with another quantity (zero or many, zero or one, thus store second quantity)\n",
        "        elif (quantity == 'zero' or quantity == 'one') and str(doc[k]) in quantity_adv:\n",
        "          snd_quantity = str(doc[k])\n",
        "\n",
        "        # if verb and quantity found, check for noun/s. compare current j position with all j positions in nouns array for this specific sentence.\n",
        "        elif verb_found and quantity_found and (doc[k].pos_ == 'NOUN' or doc[k].pos_ == 'PROPN' or str(doc[k]) == 'last' or str(doc[k]) == 'first' or str(doc[k]) == 'middle' or str(doc[k]) == 'serial'):\n",
        "          # seach in non-lemmatized nouns since the text is not lemmatized and nouns can be not found\n",
        "          for noun in nouns[i]:\n",
        "            if noun[1] == k:\n",
        "              other_subj = noun[0]\n",
        "              break\n",
        "          noun_found = True\n",
        "\n",
        "          k += len(other_subj.split(' ')) - 1\n",
        "          # check relationship cardinality -> from position 0 to 3 is one, otherwise multiple. First subject in general considered singular\n",
        "          # lemmatize noun\n",
        "          lem = nlp(other_subj)\n",
        "          lem_token = ' '.join([token.lemma_ for token in lem])\n",
        "          other_subj = lem_token\n",
        "\n",
        "          if other_subj not in common_attr:\n",
        "            if quantity == 'zero' and len(snd_quantity) > 0:\n",
        "              # zero or one\n",
        "              if snd_quantity == 'one':\n",
        "                  cardinality.append([('1','01'), subjects[i][j][0], other_subj, verb])\n",
        "              # zero or more\n",
        "              elif snd_quantity in quantity_adv[6:]:\n",
        "                cardinality.append([('1','02'), subjects[i][j][0], other_subj, verb])\n",
        "            # one or more\n",
        "            elif quantity == 'one' and len(snd_quantity) > 0 and snd_quantity in quantity_adv[5:]:\n",
        "              cardinality.append([('1','12'), subjects[i][j][0], other_subj, verb])\n",
        "            # one, many, several, more, etc.\n",
        "            elif snd_quantity == '':\n",
        "              # one-to-one relationship -> a unique, a sole, a single, one\n",
        "              if quantity in quantity_adv[1:6]:\n",
        "                cardinality.append([('1','1'), subjects[i][j][0], other_subj, verb])\n",
        "              else:\n",
        "                # one-to-many relationship\n",
        "                cardinality.append([('1','2'), subjects[i][j][0], other_subj, verb])\n",
        "          else:\n",
        "            # other_subj = winning_attr\n",
        "            if subjects[i][j][0] not in ent_attr.keys():\n",
        "                ent_attr[subjects[i][j][0]] = [other_subj]\n",
        "            else:\n",
        "              if other_subj not in ent_attr[subjects[i][j][0]]:\n",
        "                ent_attr[subjects[i][j][0]].append(other_subj)\n",
        "\n",
        "            # one, many, several, more, etc.\n",
        "            if snd_quantity == '':\n",
        "              # one-to-one relationship -> a unique, a sole, a single, one\n",
        "              if quantity in quantity_adv[1:5]:\n",
        "                if [1, subjects[i][j][0], other_subj] not in cardinality_attr:\n",
        "                  cardinality_attr.append([1, subjects[i][j][0], other_subj])\n",
        "              else:\n",
        "                # one-to-many relationship\n",
        "                if [2, subjects[i][j][0], other_subj] not in cardinality_attr:\n",
        "                  cardinality_attr.append([2, subjects[i][j][0], other_subj])\n",
        "\n",
        "          # now check if there are other entities associated (e.g. 'A professor can have more students but a single course.', 'A professor have a single course and department.')\n",
        "          # stop if subject found\n",
        "          attr_found = False\n",
        "          if noun_found:\n",
        "            other_subj = ''\n",
        "            z = k + 1\n",
        "            quantity_found = False\n",
        "            while z < len(doc):\n",
        "              # if encounter another subject, break loop and restart\n",
        "              if j + 1 < len(subjects[i]) and z == subjects[i][j+1][2]:\n",
        "                restart = True\n",
        "                quantity_found = False\n",
        "                quantity = ''\n",
        "                snd_quantity = ''\n",
        "                other_subj = ''\n",
        "                noun_found = False\n",
        "                break\n",
        "\n",
        "              elif str(doc[z]) in quantity_adv:\n",
        "                quantity_found = True\n",
        "                quantity = str(doc[z])\n",
        "\n",
        "              elif quantity == 'zero' and str(doc[z]) in quantity_adv:\n",
        "                snd_quantity = str(doc[z])\n",
        "\n",
        "              elif str(doc[z]) == 'and' or str(doc[z]) == ',':\n",
        "                z += 1\n",
        "                continue\n",
        "\n",
        "              elif doc[z].pos_ == 'NOUN' or doc[z].pos_ == 'PROPN' or str(doc[z]) == 'last' or str(doc[z]) == 'first' or str(doc[z]) == 'middle' or str(doc[z]) == 'serial':\n",
        "                # seach in non-lemmatized nouns since the text is not lemmatized and nouns can be not found\n",
        "                for noun in nouns[i]:\n",
        "                  if noun[1] == z:\n",
        "                    other_subj = noun[0]\n",
        "                    break\n",
        "\n",
        "                if other_subj != '':\n",
        "                  # lemmatize noun\n",
        "                  lem = nlp(other_subj)\n",
        "                  lem_token = ' '.join([token.lemma_ for token in lem])\n",
        "                  other_subj = lem_token\n",
        "\n",
        "                  if other_subj not in common_attr:\n",
        "                    if quantity == 'zero' and len(snd_quantity) > 0:\n",
        "                      # zero or one\n",
        "                      if snd_quantity == 'one':\n",
        "                          cardinality.append([('1','01'), subjects[i][j][0], other_subj, verb])\n",
        "                      # zero or more\n",
        "                      elif snd_quantity in quantity_adv[6:]:\n",
        "                        cardinality.append([('1','02'), subjects[i][j][0], other_subj, verb])\n",
        "                    # one or more\n",
        "                    elif quantity == 'one' and len(snd_quantity) > 0 and snd_quantity in quantity_adv[5:]:\n",
        "                      cardinality.append([('1','12'), subjects[i][j][0], other_subj, verb])\n",
        "                    # one, many, several, more, etc.\n",
        "                    elif snd_quantity == '':\n",
        "                      # one-to-one relationship -> a unique, a sole, a single, one\n",
        "                      if quantity in quantity_adv[1:6]:\n",
        "                        cardinality.append([('1','1'), subjects[i][j][0], other_subj, verb])\n",
        "                      else:\n",
        "                        # one-to-many relationship\n",
        "                        cardinality.append([('1','2'), subjects[i][j][0], other_subj, verb])\n",
        "                  else:\n",
        "                    # other_subj = winning_attr\n",
        "                    if subjects[i][j][0] not in ent_attr.keys():\n",
        "                        ent_attr[subjects[i][j][0]] = [other_subj]\n",
        "                    else:\n",
        "                      if other_subj not in ent_attr[subjects[i][j][0]]:\n",
        "                        ent_attr[subjects[i][j][0]].append(other_subj)\n",
        "\n",
        "                    # one, many, several, more, etc.\n",
        "                    if snd_quantity == '':\n",
        "                      # one-to-one relationship -> a unique, a sole, a single, one\n",
        "                      if quantity in quantity_adv[1:6]:\n",
        "                        if [1, subjects[i][j][0], other_subj] not in cardinality_attr:\n",
        "                          cardinality_attr.append([1, subjects[i][j][0], other_subj])\n",
        "                      else:\n",
        "                        # one-to-many relationship\n",
        "                        if [2, subjects[i][j][0], other_subj] not in cardinality_attr:\n",
        "                          cardinality_attr.append([2, subjects[i][j][0], other_subj])\n",
        "                  z += len(other_subj.split(' ')) - 1\n",
        "                  other_subj = ''\n",
        "                  winning_attr = ''\n",
        "              z += 1\n",
        "              k += z\n",
        "        k += 1\n",
        "      j += 1\n",
        "  return ent_attr, cardinality_attr, cardinality"
      ],
      "metadata": {
        "id": "MskBrUegDhEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract attributes in sentence structure:\n",
        " - attribute's noun + 'be' + 'unique'\n",
        "\n",
        "e.g. 'The student's id is unique.'"
      ],
      "metadata": {
        "id": "vdqkAHUMhi1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unique_extr(sentences, cardinality_attr):\n",
        "  for i in range(0, len(sentences)):\n",
        "    doc = nlp(sentences[i])\n",
        "    # iterate over all tokens in the sentence and find a noun\n",
        "    j = 0\n",
        "    while j < len(doc):\n",
        "      for k in range(0, len(nouns[i])):\n",
        "        # find noun based on position\n",
        "        if nouns[i][k][1] == j:\n",
        "          # skip noun's length\n",
        "          j += len(nouns[i][k][0].split(' '))\n",
        "          # check for 's and go ahead\n",
        "          if j < len(doc):\n",
        "            if doc[j].dep_ == 'case':\n",
        "              j += 1\n",
        "              attr = ''\n",
        "              # find next noun which should be the attribute\n",
        "              for n in range(0, len(nouns[i])):\n",
        "                if nouns[i][n][1] == j:\n",
        "                  attr = nouns[i][n][0]\n",
        "                  j += len(nouns[i][n][0].split(' '))\n",
        "                  break\n",
        "              # check for 'be' verb + unique to have 'entity's attribute is unique'\n",
        "              if attr != '' and j + 1 < len(doc) and doc[j].pos_ == 'AUX' and str(doc[j+1]) == 'unique':\n",
        "                nn = nlp(nouns[i][k][0])\n",
        "                lem_token = ' '.join([token.lemma_ for token in nn])\n",
        "                cardinality_attr.append([1, lem_token, attr])\n",
        "                attr = ''\n",
        "            break\n",
        "      j += 1\n",
        "  return cardinality_attr"
      ],
      "metadata": {
        "id": "6w-1ZCoQhwC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract entities and attributes based on heuristic rule of frequency.\n",
        "\n",
        "Nouns which are present only once in the nouns list or have frequency (among nouns) below 2% are likely to be attributes, otherwise entity types"
      ],
      "metadata": {
        "id": "ms2bHse9o-sR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def freq_extraction(nouns):\n",
        "  # list (one dimension) with all nouns\n",
        "  all_nouns = []\n",
        "  for sentence in nouns:\n",
        "    for noun in sentence:\n",
        "      all_nouns.append(noun[0])\n",
        "\n",
        "  tot_nouns = len(all_nouns)\n",
        "\n",
        "  # dictionary with noun as key and occurences,frequency as values\n",
        "  noun_freq = {}\n",
        "  for i in range(0, len(all_nouns)):\n",
        "    if all_nouns[i] in noun_freq:\n",
        "      noun_freq[all_nouns[i]][0] += 1\n",
        "    else:\n",
        "      noun_freq[all_nouns[i]] = [1,0]\n",
        "\n",
        "  # add frequency at the end\n",
        "  for key, value in noun_freq.items():\n",
        "    value[1] = value[0]/tot_nouns\n",
        "  # Extract attributes if they occur only once or they frequency is less than 2%\n",
        "  freq_attr = []\n",
        "  freq_ent = []\n",
        "\n",
        "  for key, value in noun_freq.items():\n",
        "    if value[0] > 1 and value[1] >= 0.02:\n",
        "      freq_ent.append(key)\n",
        "    else:\n",
        "      freq_attr.append(key)\n",
        "  return freq_ent, freq_attr"
      ],
      "metadata": {
        "id": "Hq86rUWMpFhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract entities and attributes based on three heuristic rules:\n",
        "1. Possessive -> car's serial number - cars' serial number -> car as entity and serial number as attribute\n",
        "2. noun + (can or may) have + nouns -> first is entity, others are attributes. If there is a quantity, add the two in the cardinality array\n",
        "3. db noun + (can or may) have + nouns -> the first is not considered since associated with db (e.g. 'database', 'system', etc.),others can be entity types"
      ],
      "metadata": {
        "id": "izPn6Po7vMGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# common nouns associated to db\n",
        "db_nouns = ['database', 'db', 'system', 'organization', 'application', 'information']\n",
        "\n",
        "def attr_ent_extraction(sentences, ent_attr, cardinality):\n",
        "  # do not execute if no nouns have been found\n",
        "  if len(nouns) > 0:\n",
        "    for idx, sentence in enumerate(sentences):\n",
        "      doc = nlp(sentence)\n",
        "      i = 0\n",
        "      increment = 0\n",
        "      rule1 = False\n",
        "      rule2 = False\n",
        "      rule3 = False\n",
        "      while i < len(doc):\n",
        "        elem = ''\n",
        "        # 3. HAVE CASE WITH DB\n",
        "        if str(doc[i]) in db_nouns:\n",
        "          rule3 = True\n",
        "        # find current token in first token of all nouns extracted (check string equality and position)\n",
        "        for elems in lemmatized_nouns[idx]:\n",
        "          # find current token in nouns using position in sentence, thus no need to compare tokens\n",
        "          if i == elems[1]:\n",
        "            # if found, move i to the position of the last token (since it is the one having as .dep_ 'poss' and next token ''s' will be 'case') -> pick length stored of noun from nouns array\n",
        "            noun_len = len(elems[0].split(' '))\n",
        "            i += noun_len - 1\n",
        "            # 1. POSSESSIVE CASE -> check if last token has .dep_ 'poss' and next token is 'case' = 's (and if next token exists)\n",
        "            if doc[i].dep_ == 'poss' and i+1 < len(doc) and doc[i+1].dep_ == 'case':\n",
        "              elem = elems[0]\n",
        "              increment = 2\n",
        "              rule1 = True\n",
        "              # !! insert found noun in attribute dict as entity if not present\n",
        "              if elems[0] not in ent_attr.keys():\n",
        "                ent_attr[elem] = []\n",
        "              break\n",
        "            # 2. (CAN or MAY) HAVE CASE\n",
        "            elif i+1 < len(doc):\n",
        "              # put token to infinite\n",
        "              lemm_token = doc[i+1].lemma_\n",
        "              # if token = have verb, subsequent nouns are attributes\n",
        "              if lemm_token == 'have' or ((lemm_token == 'can' or lemm_token == 'may') and i+2 < len(doc) and doc[i+2].lemma_ == 'have'):\n",
        "                if lemm_token == 'have':\n",
        "                  increment += 2\n",
        "                else:\n",
        "                  increment += 3\n",
        "                elem = elems[0]\n",
        "                if elem not in db_nouns:\n",
        "                  if elem not in ent_attr.keys():\n",
        "                    ent_attr[elem] = []\n",
        "                rule2 = True\n",
        "                break\n",
        "        # noun with possible associated attributes has been found or not\n",
        "        if rule1 or rule2 or rule3:\n",
        "          # move i to the position of the next-next token (token after 's)\n",
        "          i += increment\n",
        "          # pick sequent noun word from nouns array and check if that noun is the one immediately after 's (check only first token)\n",
        "          j = i\n",
        "          while j < len(doc):\n",
        "            # if list of attributes, jump ',' or 'and'. If rule 2, there can be determinants before nouns or a certain amount or one of the abbreviations (this condition cannot take into consideration ca.)\n",
        "            if str(doc[j]) == ',' or str(doc[j]) == 'and' or ((rule2 or rule3) and (doc[j].pos_ == 'DET' or doc[j].pos_ == 'NUM' or doc[j].pos_ == 'ADJ' or str(doc[j]) in abbreviations)):\n",
        "              j += 1\n",
        "            # special case for ca. since it is seen as two tokens 'ca' and '.'\n",
        "            elif str(doc[j]) == 'ca' and j+1 < len(doc) and str(doc[j+1]) == '.':\n",
        "              j += 2\n",
        "            elif doc[j].pos_ == 'NOUN' or doc[j].pos_ == 'PROPN' or str(doc[j]) == 'last' or str(doc[j]) == 'first' or str(doc[j]) == 'middle' or str(doc[j]) == 'serial':\n",
        "              attr = ''\n",
        "              # check if noun in extracted nouns in specific sentence to get it as attribute -> compare only position in sentence s.t. it is more performant\n",
        "              for elems in lemmatized_nouns[idx]:\n",
        "                if j == elems[1]:\n",
        "                  attr = elems[0]\n",
        "                  break\n",
        "              # insert noun in attribute list and move j forward depending on the length of the noun to get the next attributes if any\n",
        "              if attr != '':\n",
        "                # if rule3, attributes in reality are the entities associated with the db\n",
        "                if (rule3):\n",
        "                  if attr not in ent_attr.keys():\n",
        "                    ent_attr[attr] = []\n",
        "                else:\n",
        "                  if attr not in ent_attr[elem]:\n",
        "                    ent_attr[elem].append(attr)\n",
        "              j += len(attr.split(' '))\n",
        "\n",
        "            # if attributes finished, update position of i at position of j and close loop and continue again with i (scanning for other nouns followed by attributes)\n",
        "            else:\n",
        "              i = j + 1\n",
        "              break\n",
        "          rule1 = False\n",
        "          rule2 = False\n",
        "          rule3 = False\n",
        "          increment = 0\n",
        "        i+= 1\n",
        "  return ent_attr, cardinality"
      ],
      "metadata": {
        "id": "qxKBstbU-LQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract entities and attributes based on the heuristic rule of possessive:\n",
        "1. Possessive -> noun + of + noun -> first is the attribute, second is/are the entities, e.g. 'the name of the student', 'the names of the students and the customers'. If any, require to modify the entities extracted if noun is subject of the sentence, since 'the name of the student', name is the subject but it is the attribute of student. It is required to pay attention to some cases in which the 'of' doesn't specify an entity and an attribute:\n",
        "- number of -> number of students, number of pages, etc.\n",
        "- level of -> level of education, level of complexity, etc."
      ],
      "metadata": {
        "id": "u4cnjrZT-fmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attr_ent_extraction_possessive(sentences, ent_attr):\n",
        "  # do not execute if no nouns have been found\n",
        "  if len(nouns) > 0:\n",
        "    for idx, sentence in enumerate(sentences):\n",
        "      doc = nlp(sentence)\n",
        "      i = 0\n",
        "      subject = ''\n",
        "      attribute = ''\n",
        "      rule1 = False\n",
        "      while i < len(doc):\n",
        "        elem = ''\n",
        "        # find current token in first token of all nouns extracted (check string equality and position)\n",
        "        for elems in lemmatized_nouns[idx]:\n",
        "          # find current token in nouns using position in sentence, thus no need to compare tokens\n",
        "          if i == elems[1]:\n",
        "            # if found, move i to the position of the last token (since it is the one having as .dep_ 'poss' and next token ''s' will be 'case') -> pick length stored of noun from nouns array\n",
        "            noun_len = len(elems[0].split(' '))\n",
        "            i += noun_len - 1\n",
        "            elem = elems[0]\n",
        "            # RULE 1. POSSESSIVE WITH 'OF'\n",
        "            if i+1 < len(doc) and str(doc[i+1]).lower() == 'of':\n",
        "              rule1 = True\n",
        "              attribute = elem\n",
        "              # remove noun before 'of' from entities (ent_attr dict) and save it as attribute of the next noun/s\n",
        "              if elem in ent_attr.keys():\n",
        "                del ent_attr[elem]\n",
        "              break\n",
        "\n",
        "        # if attribute associated with entity found\n",
        "        if rule1:\n",
        "          # move i to the position of the next-next token (token after of)\n",
        "          i += 2\n",
        "          # pick sequent noun word from nouns array and check if that noun is the one immediately after 's (check only first token)\n",
        "          j = i\n",
        "          found = False\n",
        "          noun_found = 0\n",
        "          while j < len(doc):\n",
        "            if noun_found == 2:\n",
        "              noun_found = 0\n",
        "              break\n",
        "            if found:\n",
        "              found = False\n",
        "              break\n",
        "            # if list of attributes, jump ',' or 'and' or determinants.\n",
        "            if doc[j].pos_ == 'DET' or doc[j].pos_ == 'CCONJ' or str(doc[j]) == ',' or str(doc[j]).lower() == 'of':\n",
        "              j += 1\n",
        "            elif doc[j].pos_ == 'NOUN' or doc[j].pos_ == 'PROPN' or str(doc[j]) == 'last' or str(doc[j]) == 'first' or str(doc[j]) == 'middle' or str(doc[j]) == 'serial':\n",
        "              # check if noun in extracted nouns in specific sentence to get it as attribute -> compare only position in sentence s.t. it is more performant\n",
        "              for elems in lemmatized_nouns[idx]:\n",
        "                if j == elems[1]:\n",
        "                  entity = elems[0]\n",
        "                  noun_found = 1\n",
        "                  j += len(entity.split(' '))\n",
        "                  # attributes and not entity-attribute relationship (number of copies, level of complexity, type of people, kind of people, piece of masterpiece, amount of money)\n",
        "                  if attribute == 'number' or attribute == 'level' or attribute == 'type' or attribute == 'kind' or attribute == 'piece' or attribute == 'amount' or attribute == 'degree' or attribute == 'quality' or attribute == 'variety' or attribute == 'state':\n",
        "                    attr = attribute + 'of' + entity\n",
        "                  # nouns which are not attributes or nouns -> example of problems, sort of problem, etc.\n",
        "                  elif attribute == 'example' or attribute == 'sort':\n",
        "                    found = True\n",
        "                    noun_found = 0\n",
        "                    break\n",
        "                  else:\n",
        "                    # update attributes dict with the new entity and the associated attribute found previously\n",
        "                    if entity not in ent_attr.keys():\n",
        "                      ent_attr[entity] = [attribute]\n",
        "                    else:\n",
        "                      ent_attr[entity].append(attribute)\n",
        "                  found = True\n",
        "                  noun_found = 0\n",
        "                  break\n",
        "              if noun_found == 0:\n",
        "                noun_found = 2\n",
        "                break\n",
        "\n",
        "            # if attributes finished, update position of i at position of j and close loop and continue again with i (scanning for other nouns followed by attributes)\n",
        "            else:\n",
        "              i = j\n",
        "              break\n",
        "          rule1 = False\n",
        "        i+= 1\n",
        "  return ent_attr"
      ],
      "metadata": {
        "id": "9CU0ZCk8-ri3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract entities and attributes based on one heuristic rules:\n",
        "\n",
        "Multiple attributes appearing in a sentence will be linked to the first entity that appears in that sentence (applicable if only one entity is present in that sentence). Generally, the first entity is the subject.\n",
        "\n",
        "Find subject, and if there are subsequent nouns, then they're attributes associated to that subject entity."
      ],
      "metadata": {
        "id": "Fo5yV_cM_MAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- ER ATTRIBUTES + ENTITIES EXTRACTION 3 --------------------\n",
        "def attr_ent_extraction3(sentences, ent_attr, cardinality_attr, composite_attr):\n",
        "  # do not execute if no nouns have been found\n",
        "  if len(subjects) > 0:\n",
        "    for i in range(len(sentences)):\n",
        "      doc = nlp(sentences[i])\n",
        "      j = 0\n",
        "      # iterate over subjects\n",
        "      while j < len(subjects[i]):\n",
        "        is_attr = False\n",
        "        found = False\n",
        "        # for each subject, analyse tokens after the subject\n",
        "        k = subjects[i][j][2] + 1\n",
        "        for key, values in ent_attr.items():\n",
        "          for value in values:\n",
        "            if value == subjects[i][j][0]:\n",
        "              found = True\n",
        "              break\n",
        "        # check if subject is an attributem, thus subsequent nouns are simple attribute of this composite attribute\n",
        "        if subjects[i][j][0] in common_attr or found:\n",
        "          is_attr = True\n",
        "        positions = [x[2] for x in subjects[i]]\n",
        "        while k < len(doc):\n",
        "          # get positions of all subjects in the current sentence. If current k correspond to position of one entity, skip everything and start with new subject\n",
        "          if k in positions:\n",
        "            break\n",
        "          noun = ''\n",
        "          for elems in nouns[i]:\n",
        "            if k == elems[1]:\n",
        "              noun = elems[0]\n",
        "              k += len(noun.split(' ')) - 1\n",
        "          if noun != '':\n",
        "            # lemmatize noun\n",
        "            nn = nlp(noun)\n",
        "            lem_token = ' '.join([token.lemma_ for token in nn])\n",
        "\n",
        "            # if subject is in db nouns, then other nouns are entities, otherwise other nouns are attributes\n",
        "            if subjects[i][j][0] in db_nouns:\n",
        "              if subjects[i][j][0] not in ent_attr.keys():\n",
        "                ent_attr[subjects[i][j][0]] = []\n",
        "            # if subject is attribute, then it is a composite attribute formed by the subsequent nouns -> name is composed by first name, second name.\n",
        "            else:\n",
        "              # check for composite nouns\n",
        "              if is_attr:\n",
        "                if subjects[i][j][0] not in composite_attr.keys():\n",
        "                  composite_attr[subjects[i][j][0]] = [lem_token]\n",
        "                else:\n",
        "                  composite_attr[subjects[i][j][0]].append(lem_token)\n",
        "              elif subjects[i][j][0] not in ent_attr.keys():\n",
        "                ent_attr[subjects[i][j][0]] = [lem_token]\n",
        "              else:\n",
        "                if lem_token not in ent_attr[subjects[i][j][0]]:\n",
        "                  ent_attr[subjects[i][j][0]].append(lem_token)\n",
        "\n",
        "\n",
        "              # if attribute is plural, then it is multivalued and insert in cardinality_attr. For composite nouns check last token\n",
        "                last_tok = nn[-1]\n",
        "                if len(last_tok.morph.get('Number')) > 0 and last_tok.morph.get('Number')[0] == 'Plur':\n",
        "                  elem = [2, subjects[i][j][0], lem_token]\n",
        "                  if elem not in cardinality_attr:\n",
        "                    cardinality_attr.append(elem)\n",
        "            noun = ''\n",
        "          k += 1\n",
        "        j += 1\n",
        "\n",
        "  return ent_attr, cardinality_attr, composite_attr"
      ],
      "metadata": {
        "id": "IRpwP4Kg_MkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unique attribute extraction\n",
        "\n",
        "If an entity doesn't have a unique attribute and contains attributes which usually are considered unique (id, name, ssn), select them as unique attributes"
      ],
      "metadata": {
        "id": "Dv_xWHTJI-B1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_unique_attr = ['id', 'identification number', 'ssn', 'code', 'name']"
      ],
      "metadata": {
        "id": "O9r8LQxRKC-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_unique(ent_attr, cardinality_attr):\n",
        "  # check if entity not in cardinality attr\n",
        "  for key, values in ent_attr.items():\n",
        "    found = False\n",
        "    for elem in cardinality_attr:\n",
        "      if key in elem and elem[0] == 1:\n",
        "        found = True\n",
        "        break\n",
        "    # if not found, check in its attributes if one is in the list of common unique attributes\n",
        "    if not found:\n",
        "      attributes = []\n",
        "      for attr in common_unique_attr:\n",
        "        if attr in values or attr.lower() in values:\n",
        "          attributes.append(attr)\n",
        "      # if there is an attribute/s in the list of attributes which can be used as unique, select it and add in cardinality_attr\n",
        "      # since only one can be selected, select them in order: id, ssn, code, name\n",
        "      if len(attributes) != 0:\n",
        "        if 'id' in attributes or 'ID' in attributes:\n",
        "          cardinality_attr.append([1, key, 'ID'])\n",
        "        elif 'identification number' in attributes:\n",
        "          cardinality_attr.append([1, key, 'identification number'])\n",
        "        elif 'ssn' in attributes:\n",
        "          cardinality_attr.append([1, key, 'ssn'])\n",
        "        elif 'code' in attributes:\n",
        "          cardinality_attr.append([1, key, 'code'])\n",
        "        elif 'name' in attributes:\n",
        "          cardinality_attr.append([1, key, 'name'])\n",
        "  return cardinality_attr"
      ],
      "metadata": {
        "id": "ph5dVmQeI9oQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extraction methods cleaning"
      ],
      "metadata": {
        "id": "vWGB_e44_SW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove all nouns associated with db both from entities and attributes"
      ],
      "metadata": {
        "id": "D2sokRVpAX_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def db_nouns_clean(dictionary):\n",
        "  dic_copy = dictionary.copy()\n",
        "  for key, value in dic_copy.items():\n",
        "    if key in db_nouns:\n",
        "      del dictionary[key]\n",
        "    else:\n",
        "      for elem in value:\n",
        "        if elem in db_nouns:\n",
        "          dictionary[key].remove(elem)"
      ],
      "metadata": {
        "id": "wmoqfA-ZAaMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove nouns from attributes list if in list of attributes but also key (entity) in ent_attr"
      ],
      "metadata": {
        "id": "Fpix5y6__SeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ent_attr_clean(dictionary):\n",
        "  dict_new = {}\n",
        "  for key, values in dictionary.items():\n",
        "    list_attr = values.copy()\n",
        "    for i in range(0, len(values)):\n",
        "      if values[i] in dictionary.keys():\n",
        "        list_attr.remove(values[i])\n",
        "    dict_new[key] = list_attr\n",
        "  # if word combined by name of entity and attribute and also present uniquely as entity, remove it as attribute\n",
        "  # e.g. {'car': ['owner'], 'car owner': []}, remove 'owner' from 'car'\n",
        "  dict_new2 = {}\n",
        "  for key, values in dict_new.items():\n",
        "    list_attr = values\n",
        "    for value in values:\n",
        "      composite_nn = key + ' ' + value\n",
        "      if composite_nn in dict_new.keys():\n",
        "        list_attr.remove(value)\n",
        "    dict_new2[key] = list_attr\n",
        "  return dict_new2"
      ],
      "metadata": {
        "id": "7L00G294_R3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove from dictionary of composite attributes nouns which are not composite attributes but are in the list of entities"
      ],
      "metadata": {
        "id": "tacYFlqeV32m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def composite_attr_clean(dictionary, ent_attr):\n",
        "  dict_new = {}\n",
        "  for key, values in dictionary.items():\n",
        "    if key not in ent_attr.keys():\n",
        "      dict_new[key] = values\n",
        "  return dict_new"
      ],
      "metadata": {
        "id": "d88cT3OfV1HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove from the array of cardinalities for the attributes the elements in which both are entities.\n",
        "\n",
        "e.g. [[2, 'project', 'professor'], [2, 'professor', 'phone number']] -> [2, 'professor', 'phone number']]"
      ],
      "metadata": {
        "id": "uGdZodaV72YH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def card_attr_clean(attrs, ent_attr):\n",
        "  new_attrs = []\n",
        "  for elem in attrs:\n",
        "    # first noun is always an entity, check if second one is an entity. If yes, remove\n",
        "    if elem[2] not in ent_attr.keys():\n",
        "      new_attrs.append(elem)\n",
        "  return new_attrs"
      ],
      "metadata": {
        "id": "625_l33k8T9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methods combinations"
      ],
      "metadata": {
        "id": "aOPBUdAmCIRO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different combinations of methods will be used, and based on three parameters (precision, recall, F1 or accuracy) the best combination will be selected.\n",
        "\n",
        "The combination will be tested on a training set, and when the fine-tuning is completed, the best one will be tested on a test set (only once)."
      ],
      "metadata": {
        "id": "LPB-JDMFCK4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance measures:\n",
        "Computed only with respect to entities and attributes"
      ],
      "metadata": {
        "id": "asw1lhdrxGZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precision: true positives/ (true positives + false positives)\n",
        "\n"
      ],
      "metadata": {
        "id": "IR0MwGEGXPC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the predict_set_cardinality contains the cardinality, the two entities and the verb of the relationship.\n",
        "# For evaluation we remove the verb and the cardinality maintaing only the entities of the relationship\n",
        "\n",
        "# make copy of cardinality to modify it while checking (there can be more than one relationships between two entities and these copies should be taken into account, thus whenever one is found remove it from the list)\n",
        "\n",
        "# ent_attr is a dictionary containing entities as keys and attributes as values\n",
        "def precision(true_ent_attr, predict_ent_attr, true_set_cardinality = None, predict_set_cardinality = None, true_unique_attr = None, predict_unique_attr = None):\n",
        "  true_positives = 0\n",
        "  false_positives = 0\n",
        "\n",
        "\n",
        "  # check if key is present, if yes check for each value, otherwise false_positives increased by the key and all the values (since if the key is not present its attributes are not present as well)\n",
        "  if len(predict_ent_attr) > 0:\n",
        "    for key, values in predict_ent_attr.items():\n",
        "      if key in true_ent_attr.keys():\n",
        "        true_positives += 1\n",
        "        for value in values:\n",
        "          if value in true_ent_attr[key]:\n",
        "            true_positives += 1\n",
        "          else:\n",
        "            false_positives += 1\n",
        "      else:\n",
        "        qnt = len(predict_ent_attr[key])\n",
        "        false_positives += qnt + 1\n",
        "\n",
        "  if true_set_cardinality != None and len(true_set_cardinality) > 0:\n",
        "    card_true_copy = true_set_cardinality.copy()\n",
        "\n",
        "    for cardinality in predict_set_cardinality:\n",
        "      sub_cardinality = [cardinality[1], cardinality[2]]\n",
        "      if sub_cardinality in card_true_copy:\n",
        "        true_positives += 1\n",
        "        card_true_copy.remove(sub_cardinality)\n",
        "      else:\n",
        "        false_positives += 1\n",
        "\n",
        "  if true_unique_attr != None and len(true_unique_attr) > 0:\n",
        "    for elem in predict_unique_attr:\n",
        "      if elem in true_unique_attr:\n",
        "        true_positives += 1\n",
        "      else:\n",
        "        false_positives += 1\n",
        "\n",
        "  if true_positives + false_positives == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return true_positives / (true_positives + false_positives)"
      ],
      "metadata": {
        "id": "fAONZ8RcC-mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall: true positives / (true positives + false negatives)"
      ],
      "metadata": {
        "id": "6LryksxQDNBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recall(true_ent_attr, predict_ent_attr, true_set_cardinality = None, predict_set_cardinality = None, true_unique_attr = None, predict_unique_attr = None):\n",
        "  true_positives = 0\n",
        "  false_negatives = 0\n",
        "\n",
        "  sub_predict_set_cardinality = []\n",
        "  card_true_copy = None\n",
        "  card_predict_copy = None\n",
        "\n",
        "  if true_set_cardinality != None and len(true_set_cardinality) > 0:\n",
        "    card_true_copy = true_set_cardinality.copy()\n",
        "    for cardinality in predict_set_cardinality:\n",
        "      sub_cardinality = [cardinality[1], cardinality[2]]\n",
        "      sub_predict_set_cardinality.append(sub_cardinality)\n",
        "\n",
        "    card_predict_copy = sub_predict_set_cardinality.copy()\n",
        "\n",
        "\n",
        "  if predict_ent_attr != None and len(predict_ent_attr) > 0:\n",
        "    for key, values in predict_ent_attr.items():\n",
        "      if key in true_ent_attr.keys():\n",
        "        true_positives += 1\n",
        "        for value in values:\n",
        "          if value in true_ent_attr[key]:\n",
        "            true_positives += 1\n",
        "\n",
        "    for key, values in true_ent_attr.items():\n",
        "      if key not in predict_ent_attr.keys():\n",
        "        qnt = len(true_ent_attr[key])\n",
        "        false_negatives += qnt + 1\n",
        "      else:\n",
        "        for value in values:\n",
        "          if value not in predict_ent_attr[key]:\n",
        "            false_negatives += 1\n",
        "\n",
        "  if true_set_cardinality != None and len(true_set_cardinality) > 0:\n",
        "    for cardinality in predict_set_cardinality:\n",
        "      sub_cardinality = [cardinality[1], cardinality[2]]\n",
        "      if sub_cardinality in card_true_copy:\n",
        "        true_positives += 1\n",
        "        card_true_copy.remove(sub_cardinality)\n",
        "\n",
        "    for true_cardinality in true_set_cardinality:\n",
        "      if true_cardinality not in card_predict_copy:\n",
        "        false_negatives += 1\n",
        "      else:\n",
        "        if true_cardinality in card_predict_copy:\n",
        "          card_predict_copy.remove(true_cardinality)\n",
        "\n",
        "  if true_unique_attr != None and len(true_unique_attr) > 0:\n",
        "    for elem in predict_unique_attr:\n",
        "      if elem in true_unique_attr:\n",
        "        true_positives += 1\n",
        "\n",
        "    for elem in true_unique_attr:\n",
        "      if elem not in predict_unique_attr:\n",
        "        false_negatives += 1\n",
        "\n",
        "  if true_positives + false_negatives == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return true_positives / (true_positives + false_negatives)"
      ],
      "metadata": {
        "id": "Wz-CFm1Paq9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "F1: 2* ((precision * recall)/ (precision + recall))"
      ],
      "metadata": {
        "id": "Qk4UIMbzPTYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f1(precision, recall):\n",
        "  if precision + recall == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return 2 * ((precision * recall) / (precision + recall))"
      ],
      "metadata": {
        "id": "54LrLW01Pax3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combinations\n",
        "The pre-processing part is the same for all combinations.\n",
        "The combinations consider only few and secondary (less precise) methods, since the main ones can be used in order paying attention to conflicts.\n",
        "\n",
        "Combinations:\n",
        "1. Initial combination (without adding the few less precise methods)\n",
        "2. Initial combination + underscore\n",
        "3. Initial combination + underscore + frequency\n",
        "4. Initial combination + underscore + frequency + subjects\n",
        "\n",
        "For each of these combinations, there are two results for the measurements:\n",
        "1. an additional cleaning not-included\n",
        "2. an additional cleaning included\n",
        "\n",
        "The additional cleaning refers to the function ent_attr_clean() for the ent_attr dictionary."
      ],
      "metadata": {
        "id": "kd1E7aKzzLxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-processing (same for all combinations)"
      ],
      "metadata": {
        "id": "xRWc36RmzQBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = text\n",
        "# simplify english text through GPT\n",
        "input_text = gpt_output()\n",
        "print(input_text)\n",
        "# clean unnecessary spaces, backticks, etc.\n",
        "input_text = clean_gpt_output(input_text)\n",
        "print(input_text)\n",
        "# sentence segmentation\n",
        "input_text = sentence_segmentation_mine(input_text)\n",
        "# transform into lowercase\n",
        "input_text = to_lowercase(input_text)\n",
        "# remove pronouns if any\n",
        "subject_extraction_for_pron(input_text)\n",
        "no_pronoun_text = pronoun_substitution(input_text)\n",
        "\n",
        "# lemmatize text (not always used in the functions)\n",
        "lemmatizedText = text_lemmatization(input_text)"
      ],
      "metadata": {
        "id": "067hYLbd8Br8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d2bef7a-b39e-410d-d8b3-cfdabc4b9367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'\n",
            "\n",
            "A company operates four departments. Each department employs one or more employees. An employee may have one or more dependents. An employee may have an employment history.\n",
            "\n",
            "'\n",
            "A company operates four departments. Each department employs one or more employees. An employee may have one or more dependents. An employee may have an employment history.\n",
            "['A company operates four departments.', 'Each department employs one or more employees.', 'An employee may have one or more dependents.', 'An employee may have an employment history.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Starting combination\n",
        "Same initial methods for all combinations (the main ones can be applied in a specific order)"
      ],
      "metadata": {
        "id": "dccquJ909jAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list of non-lemmatized nouns\n",
        "nouns = nouns_extraction(input_text)\n",
        "# remove proper nouns of locations and people\n",
        "nouns = proper_noun_removal(nouns)\n",
        "\n",
        "# list of lemmatized nouns\n",
        "lemmatized_nouns = noun_lemmatization(nouns)\n",
        "\n",
        "# get lemmatized subjects of entire text\n",
        "subjects = subj_extraction(lemmatized_nouns)"
      ],
      "metadata": {
        "id": "SNdL9LaTR5vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def starting_comb():\n",
        "  # dictionary having entities as keys and list of attributes as values\n",
        "  # e.g. {'person': ['id', 'sex'], 'student': ['number']}\n",
        "  ent_attr = {}\n",
        "\n",
        "  # array having as element [cardinality, entity, attribute], for entity/attribute relationships (unique or multi-valued attributes),\n",
        "  # e.g. [1,person,id] -> a person has a unique id\n",
        "  # e.g. [2, person, phone number] -> a person has multiple phone numbers\n",
        "  cardinality_attr = []\n",
        "\n",
        "  # array having as elements [cardinality, first entity, second entity, verb of relationship] for entity/entity relationships.\n",
        "  # e.g. [('1', '2'), 'user', 'website', ' uses'] -> a user uses multiple websites\n",
        "  cardinality = []\n",
        "\n",
        "  # dictionary containing as key the entity and as values the first as the composite attribute and the others the simple attribute composing the composite one\n",
        "  # e.g. {'person': ['name', 'first name', 'last name']}\n",
        "  composite_attr = {}\n",
        "\n",
        "  # 1. basic combination of methods\n",
        "\n",
        "  # insert in ent_attr\n",
        "  # extract from composite nouns common attributes, e.g. 'person id'\n",
        "  ent_attr = extract_common(lemmatized_nouns, ent_attr)\n",
        "\n",
        "  # insert in cardinality_attr\n",
        "  # extract attributes in sentences 'attribute's entity is unique.', e.g. 'person's id is unique'\n",
        "  cardinality_attr = unique_extr(no_pronoun_text, cardinality_attr)\n",
        "\n",
        "  # insert in ent_attr\n",
        "  # extract entities and attributes if possessive form 'of', e.g. 'the name of the person'\n",
        "  ent_attr = attr_ent_extraction_possessive(lemmatizedText, ent_attr)\n",
        "\n",
        "  # insert in ent_attr, cardinality_attr, cardinality\n",
        "  # extract entities and their relationship based on quantity tokens, e.g. 'a writer has written books'\n",
        "  ent_attr, cardinality_attr, cardinality = quantity_extr(no_pronoun_text, subjects, ent_attr, cardinality_attr, cardinality)\n",
        "\n",
        "  # insert in ent_attr, cardinality\n",
        "  # extract entities based on possessive or 'have' verb\n",
        "  ent_attr, cardinality = attr_ent_extraction(lemmatizedText, ent_attr, cardinality)\n",
        "\n",
        "  # insert in ent_attr, cardinality_attr\n",
        "  # extract entities and attributes based on the fact that if a subject (entity) is followed by some nouns, these ones are its attributes\n",
        "  # result in ent_attr in conflict with ent_attr4, thus remove here the attribute which are seen as entities in the cardinality1\n",
        "  ent_attr, cardinality_attr, composite_attr = attr_ent_extraction3(no_pronoun_text, ent_attr, cardinality_attr,composite_attr)\n",
        "\n",
        "  # clean dictionary of composite attributes by removing the entities\n",
        "  composite_attr = composite_attr_clean(composite_attr, ent_attr)\n",
        "\n",
        "  return ent_attr, cardinality, cardinality_attr, composite_attr"
      ],
      "metadata": {
        "id": "dijRb-hmrjmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation basic combination without and with additional cleaning"
      ],
      "metadata": {
        "id": "TJcpAv66fBk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "str_ent_attr, str_cardinality, str_cardinality_attr, str_composite_attr = starting_comb()\n",
        "# remove nouns associated to db\n",
        "db_nouns_clean(str_ent_attr)\n",
        "\n",
        "prec = precision(true_res, str_ent_attr)\n",
        "rec = recall(true_res, str_ent_attr)\n",
        "f_score1 = f1(prec, rec)\n",
        "\n",
        "print('true dict:', true_res, '\\npredicted dict:', str_ent_attr)\n",
        "print('basic precision:', prec)\n",
        "print('basic recall:', rec)\n",
        "print('basic F1:', f_score1)"
      ],
      "metadata": {
        "id": "gWJEfapccNqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "str_ent_attr = ent_attr_clean(str_ent_attr)\n",
        "\n",
        "prec = precision(true_res, str_ent_attr)\n",
        "rec = recall(true_res, str_ent_attr)\n",
        "f_score1 = f1(prec, rec)\n",
        "\n",
        "print('true dict:', true_res, '\\npredicted dict:', str_ent_attr)\n",
        "print('basic precision with additional cleaning:', prec)\n",
        "print('basic recall with additional cleaning:', rec)\n",
        "print('basic F1 with additional cleaning:', f_score1)"
      ],
      "metadata": {
        "id": "oGX25oax_m1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combination 1\n",
        "basic combo +\n",
        "1. extract_underscore"
      ],
      "metadata": {
        "id": "9ATHsBK0yF-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ent_attr1, cardinality1, cardinality_attr1, composite_attr1 = starting_comb()\n",
        "\n",
        "ent_attr1 = extract_underscore(nouns, ent_attr1)\n",
        "db_nouns_clean(ent_attr1)\n",
        "\n",
        "prec1 = precision(true_res, ent_attr1)\n",
        "rec1 = recall(true_res, ent_attr1)\n",
        "f11 = f1(prec1, rec1)\n",
        "\n",
        "print('true dict:', true_res, '\\n predicted dict:', ent_attr1)\n",
        "print('basic precision:', prec1)\n",
        "print('basic recall:', rec1)\n",
        "print('basic F1:', f11)"
      ],
      "metadata": {
        "id": "B6z0j9Y6_jA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ent_attr1 = ent_attr_clean(ent_attr1)\n",
        "\n",
        "prec1 = precision(true_res, ent_attr1)\n",
        "rec1 = recall(true_res, ent_attr1)\n",
        "f11 = f1(prec1, rec1)\n",
        "\n",
        "print('true dict:', true_res, '\\n predicted dict:', ent_attr1)\n",
        "print('basic precision with additional cleaning:', prec1)\n",
        "print('basic recall with additional cleaning:', rec1)\n",
        "print('basic F1 with additional cleaning:', f11)"
      ],
      "metadata": {
        "id": "MBi0cvB7AZ3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combination 2\n",
        "basic combo +\n",
        "1. extract_underscore\n",
        "2. freq_extraction -> in general entities have highest frequency, thus remove entities not enough frequent"
      ],
      "metadata": {
        "id": "L9rjKwYsAx9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ent_attr2, cardinality2, cardinality_attr2, composite_attr2 = starting_comb()\n",
        "ent_attr2 = extract_underscore(nouns, ent_attr2)\n",
        "print(ent_attr2)\n",
        "freq_entities2, freq_attibutes2 = freq_extraction(nouns)"
      ],
      "metadata": {
        "id": "1dztyuyuA3pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove entities if not in frequency array (the entity doesn't have a high enough frequency to be considered as entity)"
      ],
      "metadata": {
        "id": "3YNV_JC9Qj0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_entities(dictionary, frequency):\n",
        "  dictionary_copy = dictionary.copy()\n",
        "  for key in dictionary_copy.keys():\n",
        "    if key not in frequency:\n",
        "      del dictionary[key]\n",
        "  return dictionary"
      ],
      "metadata": {
        "id": "HCMty-iPQrr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ent_attr2 = filter_entities(ent_attr2, freq_entities2)\n",
        "db_nouns_clean(ent_attr2)"
      ],
      "metadata": {
        "id": "YBi4ntLIb1Xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prec2 = precision(true_res, ent_attr2)\n",
        "rec2 = recall(true_res, ent_attr2)\n",
        "f12 = f1(prec2, rec2)\n",
        "\n",
        "print('true dict:', true_res, '\\n predicted dict:', ent_attr2)\n",
        "print('basic precision:', prec2)\n",
        "print('basic recall:', rec2)\n",
        "print('basic F1:', f12)"
      ],
      "metadata": {
        "id": "vB1vlqqhh42Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ent_attr_final2 = ent_attr_clean(ent_attr2)\n",
        "\n",
        "prec2 = precision(true_res, ent_attr2)\n",
        "rec2 = recall(true_res, ent_attr2)\n",
        "f12 = f1(prec2, rec2)\n",
        "\n",
        "print('true dict:', true_res, '\\n predicted dict:', ent_attr2)\n",
        "print('basic precision with additional cleaning:', prec2)\n",
        "print('basic recall with additional cleaning:', rec2)\n",
        "print('basic F1 with additional cleaning:', f12)"
      ],
      "metadata": {
        "id": "iJsvBIqSAh_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combination 3\n",
        "basic combo +\n",
        "1. extract_underscore\n",
        "2. freq_extraction\n",
        "3. subj_extraction -> in general entities have been also at least once the subject of a sentence"
      ],
      "metadata": {
        "id": "YDYZfHr5twcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ent_attr3, cardinality3, cardinality_attr3, composite_attr3 = starting_comb()\n",
        "ent_attr3 = extract_underscore(nouns, ent_attr3)\n",
        "\n",
        "freq_entities3, freq_attibutes3 = freq_extraction(nouns)\n",
        "\n",
        "ent_attr3 = filter_entities(ent_attr3, freq_entities3)"
      ],
      "metadata": {
        "id": "90ygGibLuGfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove entities if they have not been subjects of a sentence at least once"
      ],
      "metadata": {
        "id": "NcQ3p8ZzuU4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# take from array of subjects (structure e.g. [[['user', 'NN', 3]], [['ID', 'NNP', 3]], [['user', 'NN', 1]], [['website', 'NN', 1]]]) only subjects without repetition -> ['user', 'ID', 'website']\n",
        "def extract_subjects(subj_array):\n",
        "  subj_only = []\n",
        "  for sentence in subj_array:\n",
        "    for elem in sentence:\n",
        "      if elem[0] not in subj_only:\n",
        "        subj_only.append(elem[0])\n",
        "  return subj_only"
      ],
      "metadata": {
        "id": "I8Z1Sz71uatq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simplified_subjs = extract_subjects(subjects)"
      ],
      "metadata": {
        "id": "DxAC3qNXvNXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ent_attr3 = filter_entities(ent_attr3, simplified_subjs)\n",
        "db_nouns_clean(ent_attr3)"
      ],
      "metadata": {
        "id": "uo8rmkXhvhRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prec3 = precision(true_res, ent_attr3)\n",
        "rec3 = recall(true_res, ent_attr3)\n",
        "f13 = f1(prec3, rec3)\n",
        "\n",
        "print('true dict:', true_res, '\\n predicted dict:', ent_attr3)\n",
        "print('basic precision:', prec3)\n",
        "print('basic recall:', rec3)\n",
        "print('basic F1:', f13)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Shbxwz4viS5o",
        "outputId": "0a10b874-bf63-4c8c-984f-c6745c0b6adb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true dict: {'member': ['memdid', 'name', 'zip', 'date'], 'membership type': ['mname', 'price', 'mid'], 'sale transaction': ['date', 'tid'], 'merchandise item': ['mrchid', 'price', 'name'], 'day pass': ['ID', 'date'], 'pass category': ['passcatid', 'pcname', 'price']} \n",
            " predicted dict: {'employee': ['employment history', 'dependent']}\n",
            "basic precision: 0.0\n",
            "basic recall: 0.0\n",
            "basic F1: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ent_attr3 = ent_attr_clean(ent_attr3)\n",
        "\n",
        "prec3 = precision(true_res, ent_attr3)\n",
        "rec3 = recall(true_res, ent_attr3)\n",
        "f13 = f1(prec3, rec3)\n",
        "\n",
        "print('predicted dict:', ent_attr3)\n",
        "print('basic precision with additional cleaning:', prec3)\n",
        "print('basic recall with additional cleaning:', rec3)\n",
        "print('basic F1 with additional cleaning:', f13)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajovocfiAv4j",
        "outputId": "a2dea551-fa15-4de5-b56b-7837da27fd92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicted dict: {'employee': ['employment history', 'dependent']}\n",
            "basic precision with additional cleaning: 0.0\n",
            "basic recall with additional cleaning: 0.0\n",
            "basic F1 with additional cleaning: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combination 4\n",
        "basic combo +\n",
        "1. extract_underscore\n",
        "2. remove entities only if they're not enough frequent (freq_extraction) and they have never been subjects (subj_extraction)"
      ],
      "metadata": {
        "id": "HfvEiKyDGnYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ent_attr4, cardinality4, cardinality_attr4, composite_attr4 = starting_comb()\n",
        "ent_attr4 = extract_underscore(nouns, ent_attr4)\n",
        "\n",
        "freq_entities4, freq_attibutes4 = freq_extraction(nouns)\n",
        "\n",
        "simplified_subjs1 = extract_subjects(subjects)"
      ],
      "metadata": {
        "id": "I2pqKJccG4ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get only nouns which are both in frequency array and subject array"
      ],
      "metadata": {
        "id": "wOIozyXJHeyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge(arr1, arr2):\n",
        "  merged = []\n",
        "  for elem in arr1:\n",
        "    if elem in arr2:\n",
        "      merged.append(elem)\n",
        "      break"
      ],
      "metadata": {
        "id": "k_4AitRgHuel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_nouns = merge(freq_entities4, simplified_subjs1)"
      ],
      "metadata": {
        "id": "Ny8ge_jfIJj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "remove only entities which are not in both frequency and subject"
      ],
      "metadata": {
        "id": "8YSvnd01IMT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if common_nouns != None:\n",
        "  ent_attr4 = filter_entities(ent_attr4, common_nouns)\n",
        "db_nouns_clean(ent_attr4)"
      ],
      "metadata": {
        "id": "nqoXoSmzITgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prec4 = precision(true_res, ent_attr4)\n",
        "rec4 = recall(true_res, ent_attr4)\n",
        "f14 = f1(prec4, rec4)\n",
        "\n",
        "print('true dict:', true_res, '\\n predicted dict:', ent_attr4)\n",
        "print('basic precision:', prec4)\n",
        "print('basic recall:', rec4)\n",
        "print('basic F1:', f14)"
      ],
      "metadata": {
        "id": "2vvaHEawicRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ent_attr_final4 = ent_attr_clean(ent_attr4)\n",
        "\n",
        "prec4 = precision(true_res, ent_attr4)\n",
        "rec4 = recall(true_res, ent_attr4)\n",
        "f14 = f1(prec4, rec4)\n",
        "\n",
        "print('true dict:', true_res, '\\n predicted dict:', ent_attr4)\n",
        "print('basic precision with additional cleaning:', prec4)\n",
        "print('basic recall with additional cleaning:', rec4)\n",
        "print('basic F1 with additional cleaning:', f14)"
      ],
      "metadata": {
        "id": "UJyXc0TmA5GZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation combinations using training set"
      ],
      "metadata": {
        "id": "uIM8p7aTZlDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results (precision, recall and F1 score for each sample in training set)"
      ],
      "metadata": {
        "id": "kqsI_oI47vRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "basic = [([0.5, 0.487, 0.493], [0.594, 0.487, 0.535]), ([0.75, 1.0, 0.857], [0.875, 1.0, 0.933]), ([0.533, 0.667, 0.593], [0.571, 0.667, 0.615]), ([0.413, 0.95, 0.576], [0.542, 0.95, 0.691]),([0.8, 1.0, 0.889], [1.0, 1.0, 1.0]),([0.375, 0.75, 0.5], [0.426, 0.75, 0.545]),([0.714, 0.833, 0.769], [0.833, 0.833, 0.833]),([0.474, 0.75, 0.581], [0.533, 0.667, 0.592]),([0.3, 1.0, 0.461], [0.5, 1.0, 0.667]),([0.714, 0.909, 0.8], [0.833, 0.909, 0.869]),([0.778, 1.0, 0.875], [0.875, 1.0, 0.933]),([0.75, 0.947, 0.837], [0.857, 0.947, 0.9]),([0.538, 0.636, 0.583], [0.7, 0.636, 0.667]),([0.7, 0.913, 0.792], [0.84, 0.913, 0.875]),([0.667, 1.0, 0.8], [0.8, 1.0, 0.889]),([0.857, 1.0, 0.923], [1.0, 1.0, 1.0]),([0.55, 0.846, 0.667], [0.611, 0.846, 0.71]),([0.5, 0.75, 0.6], [0.75, 0.75, 0.75]),([0.64, 0.727, 0.682], [0.75, 0.682, 0.714])]\n",
        "one = [([0.5, 0.487, 0.493], [0.594, 0.487, 0.535]), ([0.75, 1.0, 0.857], [0.875, 1.0, 0.933]), ([0.421, 0.667, 0.516], [0.444, 0.667, 0.533]),([0.413, 0.95, 0.576], [0.542, 0.95, 0.691]),([0.8, 1.0, 0.889], [1.0, 1.0, 1.0]),([0.375, 0.75, 0.5], [0.426, 0.75, 0.545]),([0.714, 0.833, 0.769], [0.833, 0.833, 0.833]),([0.391, 0.75, 0.514], [0.47, 0.667, 0.552]),([0.3, 1.0, 0.461], [0.5, 1.0, 0.667]),([0.714, 0.909, 0.8], [0.833, 0.909, 0.869]),([0.778, 1.0, 0.875], [0.875, 1.0, 0.933]),([0.75, 0.947, 0.837], [0.857, 0.947, 0.9]),([0.538, 0.636, 0.583], [0.7, 0.636, 0.667]),([0.7, 0.913, 0.792], [0.84, 0.913, 0.875]),([0.667, 1.0, 0.8], [0.8, 1.0, 0.889]),([0.6, 1.0, 0.75], [0.67, 1.0, 0.8]),([0.55, 0.846, 0.667], [0.611, 0.846, 0.71]),([0.5, 0.75, 0.6], [0.75, 0.75, 0.75]),([0.64, 0.727, 0.681], [0.75, 0.682, 0.714])]\n",
        "two = [([0.5, 0.487, 0.493 ], [0.5, 0.487, 0.493]),([0.808, 1.0, 0.894], [0.808, 1.0, 0.894]), ([0.533, 0.667, 0.593], [0.533, 0.667, 0.593]),([0.421, 0.8, 0.552], [0.421, 0.8, 0.552]),([0.8, 1.0, 0.889], [0.8, 1.0, 0.889]),([0.333, 0.5, 0.4], [0.333, 0.5, 0.4]),([0.714, 0.833, 0.769], [0.714, 0.833, 0.769]),([0.529, 0.75, 0.62], [0.529, 0.75, 0.62]),([0.3, 1.0, 0.461], [0.3, 1.0, 0.461]),([0.714, 0.909, 0.8], [0.714, 0.909, 0.8]),([0.778, 1.0, 0.875], [0.778, 1.0, 0.875]),([0.75, 0.947, 0.837], [0.75, 0.947, 0.837]),([0.7, 0.636, 0.667], [0.7, 0.636, 0.667]),([0.75, 0.913, 0.823], [0.75, 0.913, 0.823]),([1.0, 1.0, 1.0], [1.0, 1.0, 1.0]),([0.857, 1.0, 0.923], [0.857, 1.0, 0.923]),([0.75, 0.692, 0.72], [0.75, 0.692, 0.72]),([0.5, 0.75, 0.6], [0.5, 0.75, 0.6]),([0.64, 0.727, 0.681], [0.64, 0.727, 0.681])]\n",
        "three = [([0.5, 0.487, 0.493], [0.594, 0.487, 0.535]),([0.808, 1.0, 0.894], [0.9545, 1.0, 0.977]), ([0.533, 0.667, 0.593], [0.571, 0.667, 0.615]),([0.421, 0.8, 0.552], [0.526, 0.8, 0.627]),([0.8, 1.0, 0.889], [1.0, 1.0, 1.0]),([0.333, 0.5, 0.4], [0.333, 0.5, 0.4]),([0.714, 0.833, 0.769], [0.833, 0.833, 0.833]),([0.529, 0.75, 0.62], [0.6, 0.75, 0.667]),([0.3, 1.0, 0.461], [0.5, 1.0, 0.667]),([0.714, 0.909, 0.8], [0.833, 0.909, 0.869]),([0.778, 1.0, 0.875], [0.875, 1.0, 0.933]),([0.75, 0.947, 0.837], [0.857, 0.947, 0.9]),([0.7, 0.636, 0.667], [1.0, 0.636, 0.778]),([0.75, 0.913, 0.823], [0.913, 0.913, 0.913]),([1.0, 1.0, 1.0], [1.0, 1.0, 1.0]),([0.857, 1.0, 0.923], [1.0, 1.0, 1.0]),([0.75, 0.692, 0.72], [0.818, 0.692, 0.75]),([0.5, 0.75, 0.6], [0.75, 0.75, 0.75]), ([0.64, 0.727, 0.681],[0.75, 0.682, 0.714])]\n",
        "four = [([0.5, 0.487, 0.493], [0.5, 0.487, 0.493]),([0.75, 1.0, 0.857], [0.75, 1.0, 0.857]), ([0.421, 0.667, 0.516], [0.421, 0.667, 0.516]),([0.413, 0.95, 0.576], [0.413, 0.95, 0.575]),([0.8, 1.0, 0.889], [0.8, 1.0, 0.889]),([0.375, 0.75, 0.5], [0.375, 0.75, 0.5]),([0.714, 0.833, 0.769], [0.714, 0.833, 0.769]),([0.391, 0.75, 0.514], [0.391, 0.75, 0.514]),([0.3, 1.0, 0.461], [0.3, 1.0, 0.461]),([0.714, 0.909, 0.8], [0.714, 0.909, 0.8]),([0.778, 1.0, 0.875], [0.778, 1.0, 0.875]),([0.75, 0.947, 0.837], [0.75, 0.947, 0.837]),([0.538, 0.636, 0.583], [0.538, 0.636, 0.583]),([0.7, 0.913, 0.792], [0.7, 0.913, 0.792]),([0.667, 1.0, 0.8], [0.667, 1.0, 0.8]),([0.6, 1.0, 0.75], [0.6, 1.0, 0.75]),([0.55, 0.846, 0.667], [0.55, 0.846, 0.667]),([0.5, 0.75, 0.6], [0.5, 0.75, 0.6]),([0.64, 0.727, 0.681], [0.64, 0.727, 0.681])]"
      ],
      "metadata": {
        "id": "jlL-SDIZ3TAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average precision, recall and F1 score of all samples in training set"
      ],
      "metadata": {
        "id": "rPnNP7rW8DXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def avg(combination):\n",
        "  avg_res = []\n",
        "  standard = [0,0,0]\n",
        "  add_clean = [0,0,0]\n",
        "  tot_nr = len(combination)\n",
        "  for elem in combination:\n",
        "    standard[0] += elem[0][0]\n",
        "    standard[1] +=  elem[0][1]\n",
        "    standard[2] += elem[0][2]\n",
        "\n",
        "    add_clean[0] += elem[1][0]\n",
        "    add_clean[1] += elem[1][1]\n",
        "    add_clean[2] += elem[1][2]\n",
        "  standard = [x/tot_nr for x in standard]\n",
        "  add_clean = [x/tot_nr for x in add_clean]\n",
        "  avg_res.append(standard)\n",
        "  avg_res.append(add_clean)\n",
        "  return avg_res"
      ],
      "metadata": {
        "id": "EUWPyGhc8FBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_basic = avg(basic)\n",
        "print('average precision, recall and F1 of basic:',avg_basic)\n",
        "\n",
        "avg_one = avg(one)\n",
        "print('average precision, recall and F1 of one:',avg_one)\n",
        "\n",
        "avg_two = avg(two)\n",
        "print('average precision, recall and F1 of two:',avg_two)\n",
        "\n",
        "avg_three = avg(three)\n",
        "print('average precision, recall and F1 of three:',avg_three)\n",
        "\n",
        "avg_four = avg(four)\n",
        "print('average precision, recall and F1 of four:',avg_four)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA96211OMI1w",
        "outputId": "faa0b2b8-0703-4308-cb53-7c4f87ba2e40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg_basic: [[0.6080526315789473, 0.8507894736842104, 0.698842105263158], [0.7310526315789474, 0.8440526315789473, 0.7746315789473683]]\n",
            "avg_one: [[0.5842631578947369, 0.8507894736842104, 0.6821052631578948], [0.7036842105263158, 0.8440526315789473, 0.7576842105263157]]\n",
            "avg_two: [[0.6514210526315789, 0.8216315789473684, 0.7156315789473685], [0.6514210526315789, 0.8216315789473684, 0.7156315789473685]]\n",
            "avg_three: [[0.6514210526315789, 0.8216315789473684, 0.7156315789473685], [0.774078947368421, 0.8192631578947368, 0.7856842105263159]]\n",
            "avg_four: [[0.5842631578947369, 0.8507894736842104, 0.6821052631578948], [0.5842631578947369, 0.8507894736842104, 0.6820526315789474]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We take into consideration the F1 score, since the recall is always higher than the precision, thus they're not balanced. By the results above, the best combination is the third combination with the additional cleaning (0.7856842105263159 ~= 79%)"
      ],
      "metadata": {
        "id": "TjNcQtZcd01U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Best combination evaluation\n",
        "Evaluation of the best combination taking into consideration attributes, entities, relationships among entities, unique/multivalued/composite attributes (keys)"
      ],
      "metadata": {
        "id": "VCm0M0IBdkmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_ent_attr = ent_attr3\n",
        "best_cardinality = cardinality3\n",
        "# maintain unique and multivalued attributes\n",
        "best_cardinality_attr = cardinality_attr3\n",
        "# clean cardianlity_attr\n",
        "best_cardinality_attr = card_attr_clean(best_cardinality_attr, best_ent_attr)\n",
        "\n",
        "best_composite_attr = composite_attr3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISK4tfNoh8ls",
        "outputId": "1453d2d3-a26e-4014-ce6d-b8f6fdfa0d6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "employee ['employment history', 'dependent']\n",
            "\n",
            "\n",
            "unique:\n",
            " [[2, 'employee', 'dependent']]\n",
            "relation:\n",
            " [[('1', '12'), 'department', 'employee', 'employs'], [('1', '12'), 'employee', 'dependent', 'have']]\n",
            "composite:\n",
            " {'department': ['employee']}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[8, 5, 3],\n",
              " [9, 6, 1],\n",
              " [3, 2, 1],\n",
              " [6, 0, 0],\n",
              " [3, 0, 0],\n",
              " [0, 1, 3],\n",
              " [0, 1, 4],\n",
              " [4, 0, 0],\n",
              " [2, 0, 0],\n",
              " [1, 2, 1]]"
            ]
          },
          "metadata": {},
          "execution_count": 2258
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_precision = precision(true_res, best_ent_attr, true_card, best_cardinality, true_unique, best_cardinality_attr)\n",
        "best_recall = recall(true_res, best_ent_attr, true_card, best_cardinality, true_unique, best_cardinality_attr)\n",
        "best_f1 = f1(best_precision, best_recall)\n",
        "\n",
        "print('best precision:', best_precision)\n",
        "print('best recall:', best_recall)\n",
        "print('best f1 score:', best_f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQGNOHGUpVQt",
        "outputId": "4692e095-e2e8-4ded-90d4-e59feb205339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best precision: 0.0\n",
            "best recall: 0.0\n",
            "best f1 score: 0\n",
            "[0.0, 0.0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Results of best combination of all samples using training set"
      ],
      "metadata": {
        "id": "xU_Q2AUUOIND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_res = [[0.8888888888888888, 0.7441860465116279, 0.8101265822784809], [0.47058823529411764, 0.4444444444444444, 0.45714285714285713], [0.5306122448979592, 0.325, 0.40310077519379844], [0.8888888888888888, 0.7441860465116279, 0.8101265822784809], [1.0, 0.8857142857142857, 0.9393939393939393], [0.6666666666666666, 0.5, 0.5714285714285715], [0.875, 0.875, 0.875], [0.7142857142857143, 0.7692307692307693, 0.7407407407407408], [0.7333333333333333, 0.8461538461538461, 0.7857142857142856], [0.5833333333333334, 0.7, 0.6363636363636365], [0.4791666666666667, 0.7419354838709677, 0.5822784810126582], [0.7894736842105263, 0.7894736842105263, 0.7894736842105263], [0.9, 0.9310344827586207, 0.9152542372881356], [0.6875, 0.6111111111111112, 0.6470588235294118], [0.631578947368421, 0.6153846153846154, 0.6233766233766234], [0.9166666666666666, 1.0, 0.9565217391304348], [0.6, 0.8571428571428571, 0.7058823529411764], [0.7419354838709677, 0.5897435897435898, 0.6571428571428573], [0.6666666666666666, 0.5454545454545454, 0.6], [0.9, 0.9, 0.9], [0.9, 0.9, 0.9]]"
      ],
      "metadata": {
        "id": "_l-rX6oOOHV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average result"
      ],
      "metadata": {
        "id": "bJ818zrzr-8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_res_np = np.array(best_res)\n",
        "mean_results = np.mean(best_res_np, axis=0)\n",
        "mean_results[2] = f1(mean_results[0], mean_results[1])\n",
        "print('precision, recall and F1 score:',mean_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgLbwCz1sFzD",
        "outputId": "b705037a-8156-40ed-f229-a062ea3764ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.74117073 0.72929504 0.73518493]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Results of best combination of all samples - using test set"
      ],
      "metadata": {
        "id": "7adZWGoMperc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_res = [[0.5625, 0.4864864864864865, 0.5217391304347827], [0.7058823529411765, 0.4, 0.5106382978723405], [0.8235294117647058, 0.35, 0.4912280701754386], [0.32558139534883723, 0.2916666666666667, 0.30769230769230776], [1.0, 1.0, 1.0], [0.7857142857142857, 0.7333333333333333, 0.7586206896551724], [0.92, 0.8846153846153846, 0.9019607843137256], [0.875, 0.7777777777777778, 0.823529411764706], [0.6785714285714286, 0.6333333333333333, 0.6551724137931035], [0.6, 0.23076923076923078, 0.33333333333333337]]\n"
      ],
      "metadata": {
        "id": "QEOVGh5BphnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average result"
      ],
      "metadata": {
        "id": "_ty9oB6I7T_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_res_np = np.array(best_res)\n",
        "mean_results = np.mean(best_res_np, axis=0)\n",
        "mean_results[2] = f1(mean_results[0], mean_results[1])\n",
        "print('precision, recall and F1 score:',mean_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOKvoHEi7VGK",
        "outputId": "7506860e-1adb-4ddf-96d3-ad2a28048277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.72767789 0.57879822 0.64475525]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. result of best combination with separated evaluations - training set\n",
        "\n"
      ],
      "metadata": {
        "id": "wLN6SRwGRUHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def precision(tp, fp):\n",
        "  if tp + fp == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return tp/(tp + fp)"
      ],
      "metadata": {
        "id": "RVODBlAlSLJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recall(tp, fn):\n",
        "  if tp + fn == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return tp/(tp + fn)"
      ],
      "metadata": {
        "id": "mln-ALYOSLj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f1(precision,recall):\n",
        "  if precision + recall == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return 2 * ((precision * recall) / (precision + recall))"
      ],
      "metadata": {
        "id": "ardNTakzSM2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ent\n",
        "ent = [[4,0,0],[5,0,3], [6,0,0],[4,1,2],[5,2,0],[4,0,0], [3,0,1],[3,0,0],[2,0,1],[4,0,0],[3,0,0],[5,0,0],[2,1,1],[3,0,0],[3,1,0],[2,0,2],[3,0,0],[2,0,0], [1,0,0],[2,0,0],[3,0,1]]\n",
        "# attr\n",
        "attr = [[17,1,0],[11,9,9],[17,1,0],[11,4,3],[14,10,0],[6,1,1],[12,0,4],[10,2,2],[7,1,3],[16,1,0],[9,1,1],[15,0,0],[7,1,4],[9,0,0],[0,3,0],[0,3,0],[8,1,0],[5,1,0], [3,0,0],[4,0,0],[0,1,0]]\n",
        "# unique\n",
        "unique = [[3,0,1],[0,1,9],[4,0,2], [2,0,4],[4,1,1],[1,0,1],[4,0,2],[3,0,0],[3,0,0],[4,0,0],[0,0,3],[4,0,0],[0,2,0],[0,0,0],[0,0,0],[0,0,0],[3,0,0],[2,0,0],[0,0,0],[1,1,0],[0,0,0]]\n",
        "# composite\n",
        "composite = [[0,0,0], [0,2,0],[0,0,0],[0,1,0],[0,2,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,2,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,1,0]]\n",
        "# multivalued\n",
        "multivalued = [[0,1,0], [0,8,0],[0,0,0],[0,1,0],[0,5,0],[0,1,0],[1,1,0],[0,0,0],[2,0,0],[0,0,0],[0,1,0],[0,0,0],[0,0,0],[0,0,0],[0,2,0],[0,1,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]]\n",
        "# relation\n",
        "relation = [[7,2,2], [10,3,6],[5,0,0],[3,1,3],[4,3,1],[2,1,1],[5,1,0],[2,1,0],[0,2,3],[3,0,0],[1,2,1],[5,0,0],[1,1,3],[2,0,0],[4,0,1],[3,1,0],[2,1,1],[1,0,0],[1,2,0],[1,0,0],[3,0,0]]"
      ],
      "metadata": {
        "id": "pYBpCDDARYPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ent_np_prompt_train = np.array(ent)\n",
        "attr_np_prompt_train = np.array(attr)\n",
        "unique_np_prompt_train = np.array(unique)\n",
        "composite_np_prompt_train = np.array(composite)\n",
        "multivalued_np_prompt_train = np.array(multivalued)\n",
        "relation_np_prompt_train = np.array(relation)\n",
        "\n",
        "sum_ent_prompt_train = np.sum(ent_np_prompt_train, axis=0)\n",
        "sum_attr_prompt_train = np.sum(attr_np_prompt_train, axis=0)\n",
        "sum_unique_prompt_train = np.sum(unique_np_prompt_train, axis=0)\n",
        "sum_composite_prompt_train = np.sum(composite_np_prompt_train, axis=0)\n",
        "sum_multivalued_prompt_train = np.sum(multivalued_np_prompt_train, axis=0)\n",
        "sum_relation_prompt_train = np.sum(relation_np_prompt_train, axis=0)\n",
        "\n",
        "print('ent tp, fp and fn:',sum_ent_prompt_train)\n",
        "print('attr tp, fp and fn:',sum_attr_prompt_train)\n",
        "print('unique tp, fp and fn:',sum_unique_prompt_train)\n",
        "print('composite tp, fp and fn:',sum_composite_prompt_train)\n",
        "print('multivalued tp, fp and fn:',sum_multivalued_prompt_train)\n",
        "print('relationships tp, fp and fn:',sum_relation_prompt_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdKXyPFCR1q9",
        "outputId": "401bd879-77cc-4137-f15e-eb4eeb4cf1ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ent: [69  5 11]\n",
            "attr: [181  41  27]\n",
            "unique: [38  5 23]\n",
            "composite: [0 8 0]\n",
            "multivalued: [ 3 21  0]\n",
            "relationships: [65 21 22]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ent_prec_train = precision(sum_ent_prompt_train[0], sum_ent_prompt_train[1])\n",
        "ent_rec_train = recall(sum_ent_prompt_train[0], sum_ent_prompt_train[2])\n",
        "ent_f1_train = f1(ent_prec_train, ent_rec_train)\n",
        "\n",
        "print('ent precision, recall and F1 score:', ent_prec_train, ent_rec_train, ent_f1_train)\n",
        "\n",
        "attr_prec_train = precision(sum_attr_prompt_train[0], sum_attr_prompt_train[1])\n",
        "attr_rec_train = recall(sum_attr_prompt_train[0], sum_attr_prompt_train[2])\n",
        "attr_f1_train = f1(attr_prec_train, attr_rec_train)\n",
        "\n",
        "print('attr precision, recall and F1 score:', attr_prec_train, attr_rec_train, attr_f1_train)\n",
        "\n",
        "unique_prec_train = precision(sum_unique_prompt_train[0], sum_unique_prompt_train[1])\n",
        "unique_rec_train = recall(sum_unique_prompt_train[0], sum_unique_prompt_train[2])\n",
        "unique_f1_train = f1(unique_prec_train, unique_rec_train)\n",
        "\n",
        "print('unique precision, recall and F1 score:', unique_prec_train, unique_rec_train, unique_f1_train)\n",
        "\n",
        "composite_prec_train = precision(sum_composite_prompt_train[0], sum_composite_prompt_train[1])\n",
        "composite_rec_train = recall(sum_composite_prompt_train[0], sum_composite_prompt_train[2])\n",
        "composite_f1_train = f1(composite_prec_train, composite_rec_train)\n",
        "\n",
        "print('composite precision, recall and F1 score:', composite_prec_train, composite_rec_train, composite_f1_train)\n",
        "\n",
        "multivalued_prec_train = precision(sum_multivalued_prompt_train[0], sum_multivalued_prompt_train[1])\n",
        "multivalued_rec_train = recall(sum_multivalued_prompt_train[0], sum_multivalued_prompt_train[2])\n",
        "multivalued_f1_train = f1(multivalued_prec_train, multivalued_rec_train)\n",
        "\n",
        "print('multivalued precision, recall and F1 score:', multivalued_prec_train, multivalued_rec_train, multivalued_f1_train)\n",
        "\n",
        "relation_prec_train = precision(sum_relation_prompt_train[0], sum_relation_prompt_train[1])\n",
        "relation_rec_train = recall(sum_relation_prompt_train[0], sum_relation_prompt_train[2])\n",
        "relation_f1_train = f1(relation_prec_train, relation_rec_train)\n",
        "\n",
        "print('relation precision, recall and F1 score:', relation_prec_train, relation_rec_train, relation_f1_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmRl7A78SToW",
        "outputId": "4d5e6d39-d763-4dcc-a5ba-3695ae932f09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ent: 0.9324324324324325 0.8625 0.8961038961038962\n",
            "attr: 0.8153153153153153 0.8701923076923077 0.841860465116279\n",
            "unique: 0.8837209302325582 0.6229508196721312 0.7307692307692308\n",
            "composite: 0.0 0 0\n",
            "multivalued: 0.125 1.0 0.2222222222222222\n",
            "relation: 0.7558139534883721 0.7471264367816092 0.7514450867052023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. result of best combination with separated evaluations - test set"
      ],
      "metadata": {
        "id": "7mE5aDbbRYjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ent\n",
        "ent = [[7,0,4], [3,0,6],[4,0,0],[5,0,0],[1,0,2],[4,0,0], [2,0,5],[4,0,0],[3,0,0],[3,0,1],[1,0,4]]\n",
        "# attr\n",
        "attr = [[19,11,0],[6,8,19],[5,1,10],[27,2,0],[0,1,0],[10,0,6],[9,1,13],[21,0,0],[10,0,0],[11,0,5],[0,2,0]]\n",
        "# unique\n",
        "unique = [[0,11,0],[4,0,0],[4,0,0],[3,0,2],[0,0,0],[3,0,1],[3,0,2],[4,0,0],[3,0,0],[3,0,1],[0,0,0]]\n",
        "# composite\n",
        "composite = [[0,2,0],[0,3,0],[0,4,0],[0,0,0],[0,1,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,1,0]]\n",
        "# multivalued\n",
        "multivalued = [[0,5,0],[0,7,0],[0,0,0],[0,0,0],[0,1,0],[0,2,0],[0,1,0],[0,1,0],[0,0,0],[0,0,0],[0,1,0]]\n",
        "# relation\n",
        "relation = [[8,5,3],[9,6,1],[3,2,1],[6,0,0],[3,0,0],[0,1,3],[0,1,4],[4,0,0],[2,0,0],[1,2,1],[2,0,2]]"
      ],
      "metadata": {
        "id": "whVHRYUoRbOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ent_np_prompt_test = np.array(ent)\n",
        "attr_np_prompt_test = np.array(attr)\n",
        "unique_np_prompt_test = np.array(unique)\n",
        "composite_np_prompt_test = np.array(composite)\n",
        "multivalued_np_prompt_test = np.array(multivalued)\n",
        "relation_np_prompt_test = np.array(relation)\n",
        "\n",
        "sum_ent_prompt_test = np.sum(ent_np_prompt_test, axis=0)\n",
        "sum_attr_prompt_test = np.sum(attr_np_prompt_test, axis=0)\n",
        "sum_unique_prompt_test = np.sum(unique_np_prompt_test, axis=0)\n",
        "sum_composite_prompt_test = np.sum(composite_np_prompt_test, axis=0)\n",
        "sum_multivalued_prompt_test = np.sum(multivalued_np_prompt_test, axis=0)\n",
        "sum_relation_prompt_test = np.sum(relation_np_prompt_test, axis=0)\n",
        "\n",
        "print('ent tp, fp and fn:',sum_ent_prompt_test)\n",
        "print('attr tp, fp and fn:',sum_attr_prompt_test)\n",
        "print('unique tp, fp and fn:',sum_unique_prompt_test)\n",
        "print('composite tp, fp and fn:',sum_composite_prompt_test)\n",
        "print('multivalued tp, fp and fn:',sum_multivalued_prompt_test)\n",
        "print('relationships tp, fp and fn:',sum_relation_prompt_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6WcK3bjSXQt",
        "outputId": "e9887297-613e-4601-ccab-73d2c94a417d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ent: [37  0 22]\n",
            "attr: [118  26  53]\n",
            "unique: [27 11  6]\n",
            "composite: [ 0 11  0]\n",
            "multivalued: [ 0 18  0]\n",
            "relationships: [38 17 15]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ent_prec_test = precision(sum_ent_prompt_test[0], sum_ent_prompt_test[1])\n",
        "ent_rec_test = recall(sum_ent_prompt_test[0], sum_ent_prompt_test[2])\n",
        "ent_f1_test = f1(ent_prec_test, ent_rec_test)\n",
        "\n",
        "print('ent precision, recall and F1 score:', ent_prec_test, ent_rec_test, ent_f1_test)\n",
        "\n",
        "attr_prec_test = precision(sum_attr_prompt_test[0], sum_attr_prompt_test[1])\n",
        "attr_rec_test = recall(sum_attr_prompt_test[0], sum_attr_prompt_test[2])\n",
        "attr_f1_test = f1(attr_prec_test, attr_rec_test)\n",
        "\n",
        "print('attr precision, recall and F1 score:', attr_prec_test, attr_rec_test, attr_f1_test)\n",
        "\n",
        "unique_prec_test = precision(sum_unique_prompt_test[0], sum_unique_prompt_test[1])\n",
        "unique_rec_test = recall(sum_unique_prompt_test[0], sum_unique_prompt_test[2])\n",
        "unique_f1_test = f1(unique_prec_test, unique_rec_test)\n",
        "\n",
        "print('unique precision, recall and F1 score:', unique_prec_test, unique_rec_test, unique_f1_test)\n",
        "\n",
        "composite_prec_test = precision(sum_composite_prompt_test[0], sum_composite_prompt_test[1])\n",
        "composite_rec_test = recall(sum_composite_prompt_test[0], sum_composite_prompt_test[2])\n",
        "composite_f1_test = f1(composite_prec_test, composite_rec_test)\n",
        "\n",
        "print('composite precision, recall and F1 score:', composite_prec_test, composite_rec_test, composite_f1_test)\n",
        "\n",
        "multivalued_prec_test = precision(sum_multivalued_prompt_test[0], sum_multivalued_prompt_test[1])\n",
        "multivalued_rec_test = recall(sum_multivalued_prompt_test[0], sum_multivalued_prompt_test[2])\n",
        "multivalued_f1_test = f1(multivalued_prec_test, multivalued_rec_test)\n",
        "\n",
        "print('multivalued precision, recall and F1 score:', multivalued_prec_test, multivalued_rec_test, multivalued_f1_test)\n",
        "\n",
        "relation_prec_test = precision(sum_relation_prompt_test[0], sum_relation_prompt_test[1])\n",
        "relation_rec_test = recall(sum_relation_prompt_test[0], sum_relation_prompt_test[2])\n",
        "relation_f1_test = f1(relation_prec_test, relation_rec_test)\n",
        "\n",
        "print('relation precision, recall and F1 score:', relation_prec_test, relation_rec_test, relation_f1_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btvH9ArsSYHV",
        "outputId": "2ac20a53-ef4c-4ae5-fed0-59e27d7c0412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ent: 1.0 0.6271186440677966 0.7708333333333333\n",
            "attr: 0.8194444444444444 0.6900584795321637 0.7492063492063492\n",
            "unique: 0.7105263157894737 0.8181818181818182 0.7605633802816901\n",
            "composite: 0.0 0 0\n",
            "multivalued: 0.0 0 0\n",
            "relation: 0.6909090909090909 0.7169811320754716 0.7037037037037037\n"
          ]
        }
      ]
    }
  ]
}